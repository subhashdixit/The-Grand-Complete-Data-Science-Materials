{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.svm\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing X and Y:\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "X_train=train.drop(labels=\"phishing\",axis=1)\n",
    "X_test=test.drop(labels=\"phishing\",axis=1)\n",
    "y_train=train[[\"phishing\"]]\n",
    "y_test=test[[\"phishing\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 17:15:55,810]\u001b[0m A new study created in memory with name: no-name-ae9a75dc-17c9-4e87-8b1a-65b597e6bbd7\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:15:56,749]\u001b[0m Trial 0 finished with value: 0.9249653263976564 and parameters: {'penalty': 'l2', 'C': 4.794855119438803}. Best is trial 0 with value: 0.9249653263976564.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:15:57,601]\u001b[0m Trial 1 finished with value: 0.9249782053611574 and parameters: {'penalty': 'l2', 'C': 6.48103591732811}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:00,133]\u001b[0m Trial 2 finished with value: 0.9249782053611574 and parameters: {'penalty': 'l1', 'C': 2.496816396328135}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:00,988]\u001b[0m Trial 3 finished with value: 0.9249782053611574 and parameters: {'penalty': 'l2', 'C': 6.813402641847022}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:03,372]\u001b[0m Trial 4 finished with value: 0.9249782053611574 and parameters: {'penalty': 'l1', 'C': 9.287002019629542}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:05,659]\u001b[0m Trial 5 finished with value: 0.9249782053611574 and parameters: {'penalty': 'l1', 'C': 6.561085888275384}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:08,387]\u001b[0m Trial 6 finished with value: 0.9249782053611574 and parameters: {'penalty': 'l1', 'C': 1.614328142015946}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:09,422]\u001b[0m Trial 7 finished with value: 0.9249653263976564 and parameters: {'penalty': 'l2', 'C': 3.87536333713733}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:10,354]\u001b[0m Trial 8 finished with value: 0.9249653263976564 and parameters: {'penalty': 'l2', 'C': 3.5830519191798604}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n",
      "\u001b[32m[I 2022-08-06 17:16:12,860]\u001b[0m Trial 9 finished with value: 0.9249782053611574 and parameters: {'penalty': 'l1', 'C': 9.714387319340462}. Best is trial 1 with value: 0.9249782053611574.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    classifier_name = 'LogReg'\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    if classifier_name == 'LogReg':\n",
    "        penalty = trial.suggest_categorical('penalty', ['l2', 'l1'])\n",
    "        if penalty == 'l1':\n",
    "            solver = 'saga'\n",
    "        else:\n",
    "            solver = 'lbfgs'\n",
    "        C = trial.suggest_uniform('C', 0.01, 10)\n",
    "        classifier_obj = linear_model.LogisticRegression(penalty=penalty, C=C, solver=solver)\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    score = model_selection.cross_val_score(classifier_obj, X_train, y_train, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2', 'C': 6.48103591732811}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if array.ndim == 0:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=6.48103591732811, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=linear_model.LogisticRegression(penalty=trial.params['penalty'],C=trial.params['C'])\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17333  1845]\n",
      " [ 1022 18156]]\n",
      "0.925252893940974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.90      0.92     19178\n",
      "         1.0       0.91      0.95      0.93     19178\n",
      "\n",
      "    accuracy                           0.93     38356\n",
      "   macro avg       0.93      0.93      0.93     38356\n",
      "weighted avg       0.93      0.93      0.93     38356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=lr.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 17:17:45,990]\u001b[0m A new study created in memory with name: no-name-5f1859bb-cf3d-477f-94ac-c4c761f025a8\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:25:14,180]\u001b[0m Trial 0 finished with value: 0.9623410483435286 and parameters: {'n_estimators': 1400, 'max_depth': 13.114326748128818, 'criterion': 'gini', 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9623410483435286.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:32:45,048]\u001b[0m Trial 1 finished with value: 0.9640282473006181 and parameters: {'n_estimators': 1270, 'max_depth': 24.655875500887735, 'criterion': 'entropy', 'min_samples_split': 8, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.9640282473006181.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:36:31,647]\u001b[0m Trial 2 finished with value: 0.9620963460465201 and parameters: {'n_estimators': 740, 'max_depth': 16.8146408556763, 'criterion': 'gini', 'min_samples_split': 11, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.9640282473006181.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:40:02,979]\u001b[0m Trial 3 finished with value: 0.9602159750774734 and parameters: {'n_estimators': 630, 'max_depth': 26.225946052712658, 'criterion': 'entropy', 'min_samples_split': 20, 'min_samples_leaf': 11}. Best is trial 1 with value: 0.9640282473006181.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:45:10,496]\u001b[0m Trial 4 finished with value: 0.9659343776895336 and parameters: {'n_estimators': 960, 'max_depth': 36.49687323517067, 'criterion': 'gini', 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 4 with value: 0.9659343776895336.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:47:36,914]\u001b[0m Trial 5 finished with value: 0.962985018413962 and parameters: {'n_estimators': 470, 'max_depth': 71.3892547509487, 'criterion': 'gini', 'min_samples_split': 13, 'min_samples_leaf': 7}. Best is trial 4 with value: 0.9659343776895336.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:50:40,892]\u001b[0m Trial 6 finished with value: 0.9640153633608938 and parameters: {'n_estimators': 720, 'max_depth': 78.3730065084403, 'criterion': 'gini', 'min_samples_split': 13, 'min_samples_leaf': 5}. Best is trial 4 with value: 0.9659343776895336.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 17:55:45,254]\u001b[0m Trial 7 finished with value: 0.9565324979631845 and parameters: {'n_estimators': 1340, 'max_depth': 13.027734454090218, 'criterion': 'gini', 'min_samples_split': 5, 'min_samples_leaf': 20}. Best is trial 4 with value: 0.9659343776895336.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 18:01:03,051]\u001b[0m Trial 8 finished with value: 0.9614008626101942 and parameters: {'n_estimators': 1330, 'max_depth': 13.34499782392813, 'criterion': 'gini', 'min_samples_split': 15, 'min_samples_leaf': 4}. Best is trial 4 with value: 0.9659343776895336.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-06 18:03:53,107]\u001b[0m Trial 9 finished with value: 0.9569188748301724 and parameters: {'n_estimators': 660, 'max_depth': 14.347327628954885, 'criterion': 'entropy', 'min_samples_split': 16, 'min_samples_leaf': 19}. Best is trial 4 with value: 0.9659343776895336.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    classifier_name = 'RandomForest'\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    if classifier_name == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 200, 2000,10)\n",
    "        max_depth = int(trial.suggest_float('max_depth', 10, 100, log=True))\n",
    "        criterion = trial.suggest_categorical('criterion', ['entropy','gini'])\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n",
    "        classifier_obj = sklearn.ensemble.RandomForestClassifier(\n",
    "            n_estimators=n_estimators,criterion=criterion, max_depth=max_depth,min_samples_split=min_samples_split\n",
    "            ,min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    score = model_selection.cross_val_score(classifier_obj, X_train, y_train, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 960,\n",
       " 'max_depth': 36.49687323517067,\n",
       " 'criterion': 'gini',\n",
       " 'min_samples_split': 6,\n",
       " 'min_samples_leaf': 4}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=36.49687323517067,\n",
       "                       max_features='auto', max_leaf_nodes=None,\n",
       "                       max_samples=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=4,\n",
       "                       min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=960, n_jobs=None, oob_score=False,\n",
       "                       random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=sklearn.ensemble.RandomForestClassifier(\n",
    "            n_estimators=trial.params['n_estimators'],criterion=trial.params['criterion'], \n",
    "            max_depth=trial.params['max_depth'],min_samples_split=trial.params['min_samples_split']\n",
    "            ,min_samples_leaf=trial.params['min_samples_leaf'])\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18028  1150]\n",
      " [  542 18636]]\n",
      "0.9558869538012306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.94      0.96     19178\n",
      "         1.0       0.94      0.97      0.96     19178\n",
      "\n",
      "    accuracy                           0.96     38356\n",
      "   macro avg       0.96      0.96      0.96     38356\n",
      "weighted avg       0.96      0.96      0.96     38356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=rf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 19:40:02,606]\u001b[0m A new study created in memory with name: no-name-1d9f074d-1e10-4c66-b4ed-9ec226b72663\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 19:44:13,280]\u001b[0m Trial 0 finished with value: 0.8783809275899767 and parameters: {'kernel': 'sigmoid', 'C': 7.317147370853365, 'degree': 4.0}. Best is trial 0 with value: 0.8783809275899767.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 19:48:16,792]\u001b[0m Trial 1 finished with value: 0.8784324449368478 and parameters: {'kernel': 'sigmoid', 'C': 7.048710894257587, 'degree': 5.0}. Best is trial 1 with value: 0.8784324449368478.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 19:52:19,825]\u001b[0m Trial 2 finished with value: 0.8784453248955933 and parameters: {'kernel': 'sigmoid', 'C': 6.106913341486986, 'degree': 5.0}. Best is trial 2 with value: 0.8784453248955933.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 19:55:00,583]\u001b[0m Trial 3 finished with value: 0.9267555451198182 and parameters: {'kernel': 'linear', 'C': 0.10607906528741352, 'degree': 5.0}. Best is trial 3 with value: 0.9267555451198182.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 19:58:43,549]\u001b[0m Trial 4 finished with value: 0.9410258208433836 and parameters: {'kernel': 'poly', 'C': 7.871024596290017, 'degree': 3.0}. Best is trial 4 with value: 0.9410258208433836.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:02:14,298]\u001b[0m Trial 5 finished with value: 0.8797332575721252 and parameters: {'kernel': 'sigmoid', 'C': 0.40666359696569604, 'degree': 1.0}. Best is trial 4 with value: 0.9410258208433836.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:10:28,986]\u001b[0m Trial 6 finished with value: 0.9267684255761862 and parameters: {'kernel': 'linear', 'C': 9.729873179149141, 'degree': 3.0}. Best is trial 4 with value: 0.9410258208433836.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:13:56,366]\u001b[0m Trial 7 finished with value: 0.9267555461150628 and parameters: {'kernel': 'poly', 'C': 8.200009758400713, 'degree': 1.0}. Best is trial 4 with value: 0.9410258208433836.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:18:20,519]\u001b[0m Trial 8 finished with value: 0.9267684255761862 and parameters: {'kernel': 'linear', 'C': 2.154251730084561, 'degree': 3.0}. Best is trial 4 with value: 0.9410258208433836.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:23:30,041]\u001b[0m Trial 9 finished with value: 0.9267684255761862 and parameters: {'kernel': 'linear', 'C': 3.7552314383782015, 'degree': 3.0}. Best is trial 4 with value: 0.9410258208433836.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    classifier_name = 'SVC'\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    if classifier_name == 'SVC':\n",
    "        kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "        C = trial.suggest_uniform('C', 0.01, 10)\n",
    "        degree = trial.suggest_discrete_uniform('degree', 1, 5, 1)\n",
    "        classifier_obj = sklearn.svm.SVC(kernel=kernel,C=C,degree=degree )\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    score = model_selection.cross_val_score(classifier_obj, X_train, y_train, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'poly', 'C': 7.871024596290017, 'degree': 3.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=7.871024596290017, degree=3.0, kernel='poly')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=sklearn.svm.SVC(kernel=trial.params['kernel'],C=trial.params['C'],degree=trial.params['degree'] )\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17602  1576]\n",
      " [  775 18403]]\n",
      "0.9387058087391803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.92      0.94     19178\n",
      "         1.0       0.92      0.96      0.94     19178\n",
      "\n",
      "    accuracy                           0.94     38356\n",
      "   macro avg       0.94      0.94      0.94     38356\n",
      "weighted avg       0.94      0.94      0.94     38356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=svc.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:26:32,496]\u001b[0m A new study created in memory with name: no-name-084454d2-85ac-46c2-9e76-32e8bbb640b0\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:32,714]\u001b[0m Trial 0 finished with value: 0.9178044335946057 and parameters: {'var_smoothing': 0.00290725684117792}. Best is trial 0 with value: 0.9178044335946057.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:32,921]\u001b[0m Trial 1 finished with value: 0.9185514269134676 and parameters: {'var_smoothing': 0.09260031225860263}. Best is trial 1 with value: 0.9185514269134676.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:33,124]\u001b[0m Trial 2 finished with value: 0.9177915536358601 and parameters: {'var_smoothing': 0.00017669045503350528}. Best is trial 1 with value: 0.9185514269134676.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:33,327]\u001b[0m Trial 3 finished with value: 0.9200325554878281 and parameters: {'var_smoothing': 0.2027057857482791}. Best is trial 3 with value: 0.9200325554878281.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:33,561]\u001b[0m Trial 4 finished with value: 0.9178044340922279 and parameters: {'var_smoothing': 0.0043668032860617045}. Best is trial 3 with value: 0.9200325554878281.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:33,782]\u001b[0m Trial 5 finished with value: 0.9203287801079312 and parameters: {'var_smoothing': 0.23126489489556085}. Best is trial 5 with value: 0.9203287801079312.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:33,985]\u001b[0m Trial 6 finished with value: 0.9178559519367213 and parameters: {'var_smoothing': 0.006059693203981549}. Best is trial 5 with value: 0.9203287801079312.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:34,221]\u001b[0m Trial 7 finished with value: 0.9177786741747367 and parameters: {'var_smoothing': 0.0001506684497088129}. Best is trial 5 with value: 0.9203287801079312.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:34,427]\u001b[0m Trial 8 finished with value: 0.9179074692835923 and parameters: {'var_smoothing': 0.007438237350567489}. Best is trial 5 with value: 0.9203287801079312.\u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-06 20:26:34,677]\u001b[0m Trial 9 finished with value: 0.9178044340922279 and parameters: {'var_smoothing': 0.0041706892195760336}. Best is trial 5 with value: 0.9203287801079312.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    classifier_name = 'NaiveBayes'\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    if classifier_name == 'NaiveBayes':\n",
    "        var_smoothing=trial.suggest_float(\"var_smoothing\", 1e-4, 0.3, log=True)\n",
    "        classifier_obj = sklearn.naive_bayes.GaussianNB(var_smoothing=var_smoothing)\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    score = model_selection.cross_val_score(classifier_obj, X_train, y_train, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAEaCAYAAACBwlkvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd1RU1/428GdgAEWqDIYgVsCCJYqKhaox8aekeE2MxgSFoLGXmNhjMMVCvHptWGIBS4qJV3NNTGIsKKCiBiVGIYCAhYjCgIqCCDOz3z98PdcRlGMuA+p5Pmu5lmefMt89w+Jh73PmHJUQQoCIiEhBzGq7ACIioprG8CMiIsVh+BERkeIw/IiISHEYfkREpDgMPyIiUhyGHxERKQ7Djx5boaGh6N27d6XrVCoVtmzZUsMVKdPw4cMRFBRk0teYM2cOPDw8TPoa1UGtViMmJqa2y6BqwPAj+h+Ul5fDlPeJKCsrM9mxa8OT2p8ntW56MIYfPfGGDRuGF198sUJ7z549ERoaCuC/I4uvvvoKzZs3R506ddC7d29kZ2cb7bNnzx74+vqibt26aNiwIcLCwlBQUCCtvzsaXb58OZo2bQorKysUFxcjKCgI77zzDqZPnw6NRgM7OzsMHz4ct27dMjp2UFAQ6tevD3t7ewQGBuLYsWNGr69SqbBs2TIMGTIE9vb2eOuttwAAs2bNQuvWrWFtbY1GjRph1KhRuH79urRfTEwM1Go1YmNj0a5dO9StWxeBgYG4dOkS4uLi0LFjR9SrVw+9e/fGX3/9JbvPc+bMwfr163Hw4EGoVCqoVCpp5HPz5k1MnDgRDRs2hLW1NTp27Ijt27dLxz137hxUKhW+/PJL9OvXD/Xq1cPMmTNlfaZ3P69vv/0Wnp6esLa2Rv/+/VFUVITt27ejZcuWsLW1xeuvv270Ptz9fBYvXizV9dprr0Gr1UrbCCHwz3/+E82bN4elpSXc3d2xZMkSo9dv2rQpPvzwQ4wZMwZOTk7w9fVF06ZNodfrERYWJr0XAHD16lW8/fbbaNy4MerWrYuWLVti0aJFRn8U3a3riy++QJMmTWBnZ4dXX30V+fn5Rq+7d+9e+Pv7w9raWvoZyczMlNZ/88036NChA+rUqYOmTZti8uTJKC4ultYnJCTA19cXtra2sLW1xXPPPYfdu3fLes8VRxA9poYNGyaef/75StcBEJs3bxZCCHH48GGhUqlEVlaWtP7s2bNCpVKJhIQEIYQQERERwtraWvj6+opjx46JY8eOCR8fH9G+fXthMBiEEELs27dP1K1bVyxbtkykp6eLY8eOiaCgIOHv7y9tM2zYMGFrayv69+8vTp48KU6dOiXKy8tFYGCgsLW1FcOHDxcpKSli586dwtnZWYwfP16qafv27eLbb78VaWlp4vTp0yI8PFw4OjoKrVZr1K/69euLZcuWibNnz4q0tDQhhBCffvqpiIuLE9nZ2WLv3r2iZcuWYujQodJ+0dHRQqVSicDAQJGYmCiSkpKEh4eH8PPzE4GBgeLIkSPixIkTomXLluKNN96Q9quqzzdu3BBDhgwR3bt3F7m5uSI3N1eUlJQIg8EggoKCRGBgoIiPjxeZmZlizZo1wsLCQuzdu1cIIUR2drYAIBo2bCg2b94sMjMzjT6je0VERAh3d3ejZWtra9GvXz/x+++/iwMHDgiNRiNeeOEF0bdvX5GcnCzi4uJEgwYNxNSpU41+ZmxtbcXLL78sTp06JWJjY4WHh4d4+eWXpW1WrFgh6tSpI9asWSPS09PFqlWrhJWVlVi3bp20TZMmTYStra2IiIgQaWlp4syZMyIvL0+Ym5uLJUuWSO+FEELk5uaKBQsWiKSkJJGVlSU2b94s6tWrJzZs2GBUl52dnRg8eLD4448/xKFDh0Tjxo2NPsM9e/YIMzMzMXHiRJGcnCxSU1PFunXrRGpqqvQZOzg4iE2bNonMzExx8OBB0a5dO/H2228LIYTQ6XTC0dFRvPfeeyI9PV2kp6eL7du3i7i4uErfc6Vj+NFja9iwYcLc3FzUq1evwr97w08IIdq1aydmzZolLU+fPl14eXlJyxEREQKAyMjIkNrS0tIEALFnzx4hhBCBgYFi2rRpRjWcP39eABAnT56UarK3txc3btww2i4wMFA0adJE6HQ6qW3NmjXC0tJS3Lx5s9L+6fV64eDgILZs2SK1ARDvvPNOle/N9u3bhaWlpdDr9UKIO78Y761TCCE+//xzAUD89ttvUtvixYuFk5OTUd1V9Tk8PFwEBgYabRMbGyusrKzEtWvXjNrDwsLEq6++KoT4b/h98sknVfansvAzNzcX+fn5UtuYMWOEmZmZyMvLk9omTJggOnXqJC0PGzZM1KtXz6iu3bt3CwAiPT1dCCGEm5ubmDJlitHrT5o0STRr1kxabtKkiejVq1eFOs3NzUV0dHSV/ZkwYYLo3bu3UV0ajUaUlpZKbfPnzxcuLi7Ssp+fnwgODn7gMZs0aSJWrVpl1Hbw4EEBQBQWForCwkIBQMTGxlZZHwnBaU96rHXt2hXJyckV/t1v5MiRiI6Ohl6vh06nQ0xMDEaMGGG0jbOzs9FFFS1atIBGo0FKSgoA4Pjx41iyZAlsbGykf15eXgCAjIwMab/WrVvDxsamQg0+Pj4wNzeXln19fVFWViZNW2VnZyMkJAQeHh6ws7ODnZ0drl+/jvPnz1c4zv22b9+OgIAAuLq6wsbGBm+99RbKyspw+fJlaRuVSoV27dpJyy4uLgCA9u3bG7UVFBRAr9c/Up/vd/z4cZSVlaFhw4ZG+27ZsqXCfpX1R46GDRtCo9EY1e7i4gJnZ2ejtry8PKP9vLy8YG9vLy37+voCAFJTU1FUVIScnBwEBAQY7RMYGIhz586hpKTkkes2GAxYsGABOnToAI1GAxsbG6xevbrC59q6dWtYWVkZ9e/KlSvSclJSUqXT9wCQn5+P8+fPY/LkyUbvd9++fQEAZ8+ehaOjI4YPH44+ffqgb9++WLBgAdLS0mT1QYnUtV0A0cPUrVtX1lWAISEhmDZtGnbt2gWDwYCrV69i6NChVe4n7jkvYzAYMG3aNISEhFTY7m6QAEC9evVk1S7uuxDmpZdegkajQVRUFBo1agRLS0v4+flVuJji/uMfPXoUAwcOxIwZM7Bw4UI4OjoiMTERw4YNM9rXzMzMKHzvnpOysLCo0Ha3Nrl9vp/BYIC9vT2OHz9eYZ2lpeVD+yPXvXUDd2qvrM1gMDzyse++D3fd/1kB8utetGgR5s+fj8WLF8Pb2xu2trb417/+hV27dhltd//7olKpKrzu/XXddbePS5cuRc+ePSusd3NzAwCsXbsWEydOxK+//oo9e/Zg9uzZWLFiBUaOHCmrL0rC8KOngp2dHQYPHoy1a9fCYDDgtddeQ/369Y22yc/PR2ZmJtzd3QEA6enpKCgoQOvWrQEAnTt3xpkzZ/72JffHjx+HXq+XAujIkSPSBRUFBQVISUnBTz/9hD59+gAAcnJyKoxaKpOQkACNRoPPPvtMatu2bdvfqvF+cvpsaWkpjRTv3e/atWsoLS1F27Ztq6WW6nJ3hGdnZwcAOHz4MIA7Iy87Ozu4ubnh4MGDCA4OlvaJi4tDs2bNYG1t/dBjV/ZexMXF4f/+7/8QHh4utT1s1PwgnTp1wu7duzF+/PgK65555hk0atQIaWlpFWY07te2bVu0bdsWkydPxqhRo/DFF18w/CrBaU96aowcORI///wzdu/ejXfffbfCemtra4SFhSEpKQm//fYbhg0bhnbt2knfJfzkk0/wn//8B++99x6Sk5ORmZmJX375BeHh4UZXbT5IQUEBxo4di9TUVOzatQuzZ8/GiBEjUK9ePTg6OsLZ2Rlr165Feno6jhw5gjfffBN169at8rgtW7ZEfn4+1q9fj6ysLGzatAkrV6589DeoEnL63KxZM/z55584c+YMtFotbt++jV69eqF3794YMGAAduzYgaysLCQlJWH58uVYu3ZttdT2d6lUKgwdOhSnT59GXFwcxo4di+DgYHh6egIAZsyYIdWZkZGBNWvWYNWqVbKuRG3WrBliY2Nx6dIl6QrSli1b4sCBA4iNjUV6ejo+/PBDHD169JHrnj17Nn7++WdMmjQJp06dQlpaGmJiYqSpy7lz52LZsmX47LPPcPr0aaSlpeH777+Xgu3s2bOYNm0aEhIScP78eRw5cgTx8fHSNDYZY/jRU6NLly5o164d3N3dERgYWGH9s88+i3fffRevvfaadGn/jh07pKmmnj17Yv/+/fjjjz/g7++P9u3b47333oOtrW2F6bbKvP7667C1tYWfnx8GDx6Mfv364fPPPwdwZ0ryu+++Q2ZmJtq3b4/Q0FBMmjQJzz77bJXHfemllzBr1izMnDkT7dq1wzfffIOFCxc+4rtTOTl9Dg8PR5cuXdCjRw84Ozvj66+/hkqlws6dOzFgwABMnjwZrVq1QnBwMHbt2iWNrGuLj48P/Pz88MILL6BPnz5o06YNoqOjpfWjR4/GJ598gnnz5sHLywuRkZFYsGCB0cjtQRYtWoSkpCQ0a9ZMOvc4e/ZsBAYG4tVXX0X37t1x9epVTJgw4ZHrfvHFF/HTTz/h6NGj6Nq1K3x8fLBx40bpcwgJCcG3336LXbt2wcfHB126dMGcOXPQsGFDAHemaTMyMjB48GC0aNECr732Gnr06IEVK1Y8ci1KoBKVTXYTPYF0Oh2aNGmCyZMn4/333zdaN2fOHGzZsgVnz541yWsHBQXBw8MD69atM8nxSZ7Q0FDk5ORg7969tV0KPeZ4zo+eeAaDAXl5eVizZg1u3ryJ4cOH13ZJRPSYY/jRE+/ChQto1qwZnn32WURHRxtd5k5EVBlOexIRkeLwghciIlIchh8RESkOz/k9IS5dulTbJdQajUZjdFd+pVFy/5Xcd4D9/1/77+rq+sB1HPkREZHiMPyIiEhxGH5ERKQ4DD8iIlIchh8RESkOw4+IiBSH4UdERIrD8CMiIsXhl9yfEC+t/7O2SyAiqlGJ0/xMdmyO/IiISHEYfkREpDgMPyIiUhyGHxERKQ7Dj4iIFIfhR0REisPwIyIixWH4ERGR4jD8iIhIcRh+RESkOAw/IiJSHIYfEREpDsOPiIgUh+FHRESKw/AjIiLFYfgREZHiMPyIiEhxGH5ERKQ4DD8iIlIchh8RESkOw4+IiBSH4UdERIrD8CMiIsVh+BERkeIw/IiISHEYfkREpDgMPyIiUhyGHxERKQ7Dj4iIFIfhR0REisPwIyIixWH4ERGR4jD8iIhIcRh+RESkOAw/IiJSHIYfEREpDsOPiIgUh+FHRESKw/AjIiLFYfgREZHiMPyIiEhxGH5ERKQ4DD8iIlIchh8RESkOw4+IiBSH4UdERIrD8CMiIsVh+BERkeIw/IiISHEYfkREpDgMPyIiUhzZ4WcwGExZBxERUY2RFX4GgwEhISEoLy83dT1EREQmJyv8zMzM4Orqihs3bpi6HiIiIpNTy93Qz88PkZGR6Nu3L5ycnKBSqaR1bdu2NUlxREREpiA7/H799VcAwHfffWfUrlKpsGLFiuqtioiIyIRkh19UVJQp6yAiIqoxj/RVB51Oh9TUVBw+fBgAUFpaitLSUpMURkREZCqyR34XLlxAZGQkLCwsUFBQgB49eiAlJQUHDx7Ee++9Z8oaiYiIqpXskd/atWsxaNAgLFmyBGr1ncz08vLCn3/+abLiiIiITEF2+OXk5MDf39+orU6dOigrK6v2ooiIiExJdvg5OzsjKyvLqO3s2bNwcXGp9qKIiIhMSfY5v0GDBmHBggV44YUXoNPpsGPHDuzZswcjR440ZX1ERETVTvbIr1OnTpgxYwaKiorg5eWF/Px8fPDBB3juuedMWR8REVG1kz3yO3LkCLp3747mzZsbtScmJqJbt27VXhgREZGpyB75rV69utL2NWvWVFsxRERENaHKkd+VK1cA3HmyQ15eHoQQRussLS1NVx0REZEJVBl+EyZMkP4/fvx4o3UODg4YOHBg9VdFRERkQlWG39atWwEAERER+Pjjj01eEBERkanJPud3N/i0Wi3S09NNVhAREZGpyb7aU6vVYunSpTh37hwAYPPmzUhMTERycjJGjRplqvqIiIiqneyR3xdffIGOHTti48aN0r0927dvj1OnTpmsOCIiIlOQHX5nz55F//79YWb2312sra1RUlJiksKIiIhMRXb42dvb4/Lly0ZtOTk50Gg01V4UERGRKck+5/fyyy8jMjIS/fv3h8FgQEJCAnbs2IH+/fubsj4iIqJqJzv8evXqBRsbG+zbtw9OTk44ePAgBg0aBB8fH1PWR0REVO1khx8A+Pj4MOyIiOiJ90jhl5qaiuzsbJSWlhq1DxgwoFqLIiIiMiXZ4bdhwwYcOXIErVq1Mrqfp0qlMklhREREpiI7/OLj47Fo0SLUr1/flPUQERGZnOyvOmg0GlhYWJiyFiIiohohe+Q3atQorFmzBr6+vrC3tzda5+XlVe2FERERmYrs8MvKysLJkyeRmppa4Rl+q1atqvbCiIiITEV2+H399deYNm0a2rdvb8p6iIiITE72OT8rKytObxIR0VNBdvgNGjQIMTExuHbtGgwGg9E/IiKiJ4nsac+75/X27NlTYd3dp70TERE9CWSH34oVK0xZBxERUY2RHX7Ozs6mrIOIiKjGPNK9PX/77TekpKSgqKjIqH3cuHHVWhQREZEpyb7g5bvvvsMXX3wBg8GAxMRE2NjY4Pfff4e1tbUp6yMiIqp2skd+sbGx+PDDD9G4cWMcOHAAoaGh8PPzw7///W9T1kdERFTtZI/8iouL0bhxYwCAWq2GTqeDh4cHUlJSTFYcERGRKcge+bm4uODixYto1KgRGjVqhF9//RU2NjawsbExZX1ERETVTnb4DRo0CDdu3AAAvPXWW1i6dClKS0sxfPhwkxVHRERkCrLCz2AwwNLSEi1atAAAeHh4YPny5SYtjIiIyFRknfMzMzPD559/DrX6kb4Z8cQrLi7G7t27peUzZ85gwYIFlW67evVq5OTk1FRpRET0P5B9wUvr1q2Rnp5uylqMCCFq/b6hxcXF+PXXX2VtO2rUKLi5uZm4IiIiqg6PdIeX+fPno3PnznBycoJKpZLWDRo06IH7bdmyBc7OzujTpw8A4Ntvv4VKpUJqaiqKi4uh0+kwePBgdOnSBXl5eZg/fz7atGmD9PR0TJkypcKdZQwGA1atWoWsrCwAQM+ePfHSSy9hzpw5aNq0KbKzs1FUVISxY8fi+++/x4ULF9CjRw8MHjwYAPDjjz8iNjYWANCrVy8EBwc/sP2rr77C5cuXMWXKFLRv3x7e3t4oLS3FokWLcPHiRTRv3hzjx4+HSqXCnDlzEBISAnd3d4SEhKBfv344ceIELC0tMWXKFDg4OODy5ctYvnw5DAYDOnTogB9//BGbN2+u9H3bu3cv9u7dCwAPHG0SET3N1Go1NBqNaY4td8OysjJ06dIFAFBYWCj7BXx9fRETEyOF35EjRzBr1iwEBwfD2toaRUVFmDVrFjp37gwAuHTpEkaPHv3AC2nOnTuHwsJCLFq0CMCd0ZnUGbUaH3/8MX766ScsXLgQCxYsgI2NDcaPH4/g4GDk5+cjNjYWc+fOBQDMnDkTXl5eEEJU2j5kyBBcvHgRCxcuBHBn2jM7OxuLFy+Go6MjZs+ejbS0NLRq1cqoxtu3b8PT0xNvvvkmtmzZgn379uG1115DTEwM+vbtCz8/vypHlL1790bv3r1lv89ERE8bnU4HrVb7t/d3dXV94DrZ4TdmzJi/9eLNmjVDUVERCgsLUVRUBBsbGzg4OGDjxo1ITU2FSqVCYWEhrl+/DgDQaDTShTWVadCgAfLy8rBhwwZ4e3sbPVz3boA2btwYbm5ucHR0BAA888wzKCgowJ9//gkfHx/UqVMHAODj44PU1FTp//e33z3evTw8PODk5AQAaNq0KfLy8iqEn1qtRqdOnQAAzZs3x6lTpwBAGs0CgJ+f3wNHfUREZFqPfAXLrVu3cOPGDQghpLZnnnnmoft07doViYmJuHbtGnr06IGEhAQUFRVhwYIFUKvVGDt2LMrKygBACqAHsbGxwcKFC5GcnIxffvkFhw8floLZwsICAKBSqaT/313W6/VGNd/rQe2Vufe4ZmZmlZ6XNDc3l6aFzczMoNfrZR+fiIhMT/YFLzk5OZg6dSpCQ0Mxfvx4TJgwQfpXFV9fXxw+fBhHjx5Ft27dUFJSAnt7e6jVapw+fRr5+fmyCy4qKoLBYEC3bt0wePBgZGdny963devWOH78OG7fvo3S0lIcP34crVu3fmB73bp1cevWLdnHr4qnpyeOHj0KADh8+HC1HZeIiB6N7JHfunXr0KZNG0RERGDcuHGIiorCV1999dApyrsaNWqEW7duoX79+nB0dISfnx8iIyMxffp0NG3aFA0bNpRdcGFhIVatWiWNuIYMGSJ73+bNmyMoKAgzZ84EcOfClmbNmgHAA9tbtmyJ999/Hx06dIC3t7fs16pMaGgoli9fjh9++AHe3t68KTgRUS1RCZlzfmFhYVi7di3UajVCQ0MRExOD0tJSvP/++4iKijJ1nU+F27dvw9LSEiqVCocOHcKhQ4cwdepUWft6f7rfxNURET1eEqf51f4FLxYWFtDr9VCr1bC1tYVWq0W9evVw8+bNv12Y0mRlZWHDhg0QQqBevXoYPXp0bZdERKRIssOvVatWOHLkCIKCgtCtWzfMmzcPFhYWaNOmjcmKmzlzJsrLy43axo8fLz1d4knTunVr6WsTRERUe2RPe97LYDAgISEBpaWlCAgIqPIKTfrfcdqTiJTmsZj2vEsIgZs3b8Lf39/oLi9ERERPCtnhV1xcjA0bNiAxMRE6nQ5qtRrdunVDWFgYn+lHRERPFNnf81u5ciXKysoQGRmJTZs2ITIyEuXl5Vi5cqUp6yMiIqp2ssPvzJkzGD9+PNzc3GBlZQU3NzeMHTsWKSkppqyPiIio2skOP1dXV+Tl5Rm1abXah55QJCIiehzJPufXtm1bzJ07F/7+/tBoNNBqtYiPj0dAQAD27//vlYi9evUySaFERETVRXb4ZWRkwMXFBRkZGcjIyAAAuLi4ID093eghtww/IiJ63MkKPyEERo0aBY1GA3Nzc1PXREREZFKyzvmpVCp88MEH/F4fERE9FWRf8NK0aVPk5uaashYiIqIaIfucX5s2bTBv3jwEBgZCo9EYreN5PiIiepLIDr+0tDQ0aNAAqampFdYx/IiI6EkiO/wiIiJMWQcREVGNkX3ODwBu3LiBuLg47Ny5E8Cdp6oXFBSYpDAiIiJTkR1+KSkpmDRpEuLj47Ft2zYAwOXLl7F27VqTFUdERGQKssMvJiYGkyZNwqxZs6Tv+nl4eCAzM9NkxREREZmC7PDLz89Hu3btjNrUajX0en21F0VERGRKssPPzc0NycnJRm1//PEHGjduXO1FERERmZLsqz1DQkIQGRmJjh07oqysDF988QWSkpIwZcoUU9ZHRERU7WSHX4sWLbBw4ULEx8ejTp060Gg0mDdvHpycnExZHxERUbWTHX4AUL9+fbzyyiu4ceMGbG1tea9PIiJ6IskOv+LiYmzYsAGJiYnQ6XRQq9Xo1q0bwsLCYGNjY8oaiYiIqpXsC15WrlyJsrIyREZGYtOmTYiMjER5eTlWrlxpyvqIiIiqnezwO3PmDMaPHw83NzdYWVnBzc0NY8eORUpKiinrIyIiqnayw8/V1RV5eXlGbVqtFq6urtVeFBERkSnJPufXtm1bzJ07F/7+/tBoNNBqtYiPj0dAQAD2798vbccnPBAR0eNOdvhlZGTAxcUFGRkZyMjIAAC4uLggPT0d6enp0nYMPyIietzxkUZERKQ4ss/5bdy4EefOnTNhKURERDVD9shPr9dj7ty5sLOzg7+/P/z9/Xl3FyIieiKphBBC7sYGgwEnT55EfHw8Tpw4AU9PTwQEBKBr166oU6eOKetUPO9P91e9ERHRUyRxmh+0Wu3f3v9h30Z4pPC718WLF7Fs2TJcuHABlpaW8PX1xRtvvIH69ev/7ULpwRh+RKQ0pgy/R7q3Z0lJCRITExEfH4/z58+ja9euCA8Ph0ajwY8//oh58+bhn//8598ulIiIqCbIDr9FixYhOTkZXl5eeOGFF9ClSxdYWFhI64cOHYrQ0FBT1EhERFStZIefp6cnwsPD4eDgUOl6MzMzrF27ttoKIyIiMpUqw++jjz6SHl2UlJRU6TYff/wxAMDKyqoaSyMiIjKNKsPv/ju2rF+/HuHh4SYriIiIyNSqDL+goCCj5Y0bN1ZoIyIiepLIvsMLERHR04LhR0REilPltOfp06eNlg0GQ4W2tm3bVm9VREREJlRl+K1atcpo2cbGxqhNpVJhxYoV1V8ZERGRiVQZflFRUTVRBxERUY3hOT8iIlIchh8RESkOw4+IiBSH4UdERIrD8CMiIsVh+BERkeIw/IiISHEYfkREpDgMPyIiUhyGHxERKQ7Dj4iIFIfhR0REisPwIyIixWH4ERGR4jD8iIhIcRh+RESkOAw/IiJSHIYfEREpDsOPiIgUh+FHRESKw/AjIiLFYfgREZHiMPyIiEhxGH5ERKQ4DD8iIlIchh8RESkOw4+IiBSH4UdERIrD8CMiIsVh+BERkeIw/IiISHEYfkREpDgMPyIiUhyGHxERKQ7Dj4iIFIfhR0REisPwIyIixWH4ERGR4jD8iIhIcRh+RESkOAw/IiJSHIYfEREpDsOPiIgUh+FHRESKw/AjIiLFYfgREZHiqGu7AJLnx/BWtV1CrdFoNNBqtbVdRq1Rcv+V3HeA/TcljvyIiEhxGH5ERKQ4DD8iIlIchh8RESkOw4+IiBSH4UdERIrD8CMiIsVh+BERkeIw/IiISHFUQghR20UQERHVJI78ngDTp0+v7RJqFfuv3P4rue8A+2/K/jP8iIhIcRh+RESkOOZz5syZU9tFUNWaN29e2yXUKvZfuf1Xct8B9t9U/ecFL0REpDic9iQiIsVh+BERkeLwSe6PkeTkZERHR8NgMOD5559H//79jdaXl++8twUAABD0SURBVJdjxYoVyMrKgq2tLSZNmoQGDRrUUrXVr6r+//jjj9i3bx/Mzc1hZ2eH0aNHw9nZuZaqrV5V9f2uxMRELF68GPPnz4e7u3sNV2k6cvp/+PBhfPfdd1CpVGjSpAkmTpxYC5WaRlX912q1iIqKQnFxMQwGA4YMGQJvb+9aqrZ6rVy5EidOnIC9vT0WLVpUYb0QAtHR0Th58iSsrKwwZsyY6jkPKOixoNfrxbhx48Tly5dFeXm5+OCDD8TFixeNtvnll1/EmjVrhBBCJCQkiMWLF9dGqSYhp/9//PGHKC0tFUIIsXv37qem/3L6LoQQJSUl4qOPPhIzZ84UZ8+erYVKTUNO/y9duiSmTJkibty4IYQQ4tq1a7VRqknI6f/q1avF7t27hRBCXLx4UYwZM6Y2SjWJM2fOiMzMTDF58uRK1yclJYm5c+cKg8Eg0tLSxIwZM6rldTnt+Zg4e/YsXFxc8Mwzz0CtVqNHjx44fvy40Ta//fYbgoKCAADdunXD6dOnIZ6S65Xk9L9t27awsrICAHh6eqKwsLA2Sq12cvoOAFu3bsUrr7wCCwuLWqjSdOT0f9++fejTpw9sbGwAAPb29rVRqknI6b9KpUJJSQkAoKSkBI6OjrVRqkl4eXlJn2tlfvvtNwQEBEClUqFFixYoLi7G1atX/+fXZfg9JgoLC+Hk5CQtOzk5Vfjlfu825ubmsLa2xo0bN2q0TlOR0/977d+/Hx06dKiJ0kxOTt+zs7Oh1WrRqVOnmi7P5OT0/9KlS8jNzcXs2bMxa9YsJCcn13SZJiOn/wMHDkR8fDxGjRqF+fPn45133qnpMmtNYWEhNBqNtFzV7wa5GH6PicpGcCqV6pG3eVI9St/i4uKQlZWFV155xdRl1Yiq+m4wGLBx40YMHTq0JsuqMXI+e4PBgNzcXERERGDixIlYvXo1iouLa6pEk5LT/0OHDiEoKAirV6/GjBkzsHz5chgMhpoqsVaZ6vcew+8x4eTkhIKCAmm5oKCgwtTGvdvo9XqUlJQ8dLrgSSKn/wBw6tQp7NixA1OnTn1qpv+q6ntpaSkuXryIjz/+GGPHjkVGRgY+//xzZGZm1ka51U7OZ1+/fn106dIFarUaDRo0gKurK3Jzc2u6VJOQ0//9+/eje/fuAIAWLVqgvLz8qZn1qYqTkxO0Wq20/KDfDY+K4feYcHd3R25uLvLy8qDT6XD48GF07tzZaJtOnTrhwIEDAO5c9demTZunZuQnp//Z2dlYu3Ytpk6d+lSd86mq79bW1li/fj2ioqIQFRUFT09PTJ069am52lPOZ+/j44PTp08DAIqKipCbm4tnnnmmNsqtdnL6r9FopP7n5OSgvLwcdnZ2tVFujevcuTPi4uIghEB6ejqsra2rJfx4h5fHyIkTJ7Bx40YYDAb07NkTAwYMwNatW+Hu7o7OnTujrKwMK1asQHZ2NmxsbDBp0qSn5hcAUHX/P/30U1y4cAEODg4A7vxCmDZtWi1XXT2q6vu95syZg5CQkKcm/ICq+y+EwKZNm5CcnAwzMzMMGDAAvr6+tV12tamq/zk5OVizZg1KS0sBAG+//Taee+65Wq66eixZsgQpKSm4ceMG7O3t8cYbb0Cn0wEAXnzxRQghsH79evz++++wtLTEmDFjquVnn+FHRESKw2lPIiJSHIYfEREpDsOPiIgUh+FHRESKw/AjIiLFYfgRKcyxY8cwevRohISEIDs7u0Ze88CBA5g9e/YD18+bN0/6Dmt1MtVx/668vDy88cYb0Ov1tV2K4vGRRvRUGTt2LEaOHIn27dvXdimYM2cO/P398fzzz9d2KUY2b96Md955B126dKm2YyYlJWHbtm3IycmBhYUFOnTogLfeesvonpUPM3PmzP+5hm+//RaXL1/GhAkTqvW495s0aRJeeeUV9OrVy6j9p59+QlxcHBYsWFDtr0nVjyM/omomhHis77uYn5+PRo0a/a19K+tXYmIili1bhn79+mH9+vVYvHgx1Go1PvroI9y8efN/LfexExgYiLi4uArtcXFxCAwMrIWK6O/gyI+eWgcOHMC+ffvg7u6OAwcOwMbGBuPHj0dubi62bt2K8vJyvP3229JjoqKiomBhYYErV64gIyMDzZo1w7hx46QH5qalpSEmJgaXLl2Cq6srQkND0bJlSwB3RnktW7ZESkoKsrKy0LVrV6SmpiIjIwMxMTEICgpCeHg4oqOjcezYMZSUlMDFxQWhoaFo3bo1gDsjl5ycHFhaWuLYsWPQaDQYO3asdDcLrVaLmJgYpKamQggBX19fhIeHA7hz78cffvgB165dg4eHB959990KD/otLy/HO++8A4PBgClTpsDBwQHLly9HTk4O1q1bh3PnzqF+/foYMmSIdFeZqKgoWFpaQqvVIiUlBVOmTDEaVd+988qAAQPg7+8PALC0tMSoUaMwZcoU7Nq1C4MGDZK237BhAw4ePAhHR0eEh4ejXbt20vt37yj5Yf25ePEiYmJikJWVBbVajb59+6J58+bYsWMHAOD48eNwcXHBwoULpeMGBARgxIgR+OSTT9C4cWMAd26TNnr0aKxcuRL29vZISkrCN998g/z8fLi5uWHEiBFo0qRJhZ+rgIAAbN26Ffn5+VJNOTk5OH/+PHx9fXHixAl88803uHLlCqytrdGzZ0+88cYblf6M3j9Tcf/oNT09HZs2bUJOTg6cnZ0RGhqKNm3aVP4DT4+mWp4KSPSYGDNmjPj999+FEELExsaKQYMGif379wu9Xi++/vprMWrUKLF27VpRVlYmkpOTRUhIiLh165YQQogVK1aIkJAQcebMGVFWViY2bNggPvzwQyGEEDdu3BChoaHi4MGDQqfTifj4eBEaGiqKioqEEEJERESIUaNGiQsXLgidTifKy8tFRESE2Lt3r1F9Bw8eFEVFRUKn04mdO3eK4cOHi9u3bwshhNi6dasYMmSISEpKEnq9Xnz55Zdi5syZQog7Dzz94IMPRHR0tLh165a4ffu2SE1NFUIIcfToUTFu3Dhx8eJFodPpxLZt28SsWbMe+B4NHDhQ5ObmCiGEKC8vF+PGjRP//ve/RXl5ufjjjz9ESEiI+Ouvv6T3ZOjQoSI1NVXo9Xqp1rtycnLEwIEDxZUrVyq8ztatW6X6734WP/zwgygvLxeHDh0SQ4cOlR5Oe+979bD+lJSUiBEjRoidO3eK27dvi5KSEpGeni693tKlS41quPe4UVFR4quvvpLW/fzzz+Kzzz4TQgiRmZkpwsPDRXp6utDr9SI2NlaMGTNGlJWVVfoefvLJJ2Lbtm3S8pdffikiIyOFEEKcPn1anD9/Xuj1enHu3DkxfPhwcfToUSGEEFeuXBEDBw4UOp1OCGH883p/HwoKCkRYWJj08/D777+LsLAwcf369UprokfDaU96qjVo0AA9e/aEmZkZevTogYKCArz++uuwsLDAc889B7VajcuXL0vbe3t7w8vLCxYWFnjzzTeRnp4OrVaLEydOwMXFBQEBATA3N4efnx9cXV2RlJQk7RsUFIRGjRrB3NwcanXlkyoBAQGwtbWFubk5Xn75Zeh0Oly6dEla36pVK3h7e8PMzAwBAQE4d+4cgDsPPC0sLERISAjq1KkDS0tLtGrVCgCwd+9e/OMf/4CbmxvMzc3xj3/8A+fOnUN+fn6V709GRgZKS0vRv39/qNVqtG3bFt7e3khISJC26dKlC1q1agUzMzNYWloa7X/3yQJ377d6LwcHB6MnD9jb2yM4OFh6YKurqytOnDhRYb+H9ScpKQkODg54+eWXYWlpibp168LT07PKfgKAn58fDh06JC0fOnQIfn5+AO48LLd3797w9PSEmZkZgoKCoFarkZGRUemx7p36NBgMiI+Pl2YQ2rRpg8aNG8PMzAxNmjSBr68vUlJSZNV4r7i4OHTs2FH6eWjfvj3c3d0rfc/o0XHak55q9z794e4v7nt/UVtaWko3CwZgdIFGnTp1YGNjg6tXr6KwsLDCNKKzs7PRQzXlXNzxww8/YP/+/SgsLIRKpcKtW7cqBMS9tZWXl0Ov10Or1cLZ2Rnm5uYVjpmfn4/o6Ghs2rRJahNCVFrz/a5evQqNRgMzs//+Hfwo/bK1tQUAXLt2DQ0aNDBad+3aNWk9cOexRPc+heT+15HTn4KCgr99M/e2bduirKwMGRkZcHBwwLlz5+Dj4wPgzpTywYMH8csvv0jb63S6Bz40tWvXrli/fj3S09NRVlaGsrIyeHt7A7jzB8VXX32FCxcuQKfTQafToVu3bo9cr1arRWJiotEfWHq9ntOe1YThR3SPe5+rVlpaips3b8LR0RH169fH0aNHjbbVarVGT5O///FS9y+npqbiP//5Dz766CO4ubnBzMwMYWFhlT6s834ajQZarRZ6vb5CAGo0GqNzbo/C0dERWq0WBoNBCkCtVotnn332gf24l6urK5ycnHDkyBG8+uqrUrvBYMDRo0eNrigtLCyEEEI6nlarrfDEiqr6k5+fbzR6u1dVj/cyMzND9+7dcejQIdjb28Pb2xt169YFcCfgBwwYgAEDBjz0GHdZWVmha9euiIuLQ1lZGXr06CGN9pctW4Y+ffpgxowZsLS0RExMDIqKih54nLKyMmn52rVr0v+dnJzg7++PUaNGyaqJHg2nPYnucfLkSfz555/Q6XT45ptv4OnpCY1Gg44dOyI3NxcJCQnQ6/U4fPgwcnJypL/2K2Nvb48rV65Iy7du3YK5uTns7OxgMBiwbds2lJSUyKrLw8MDjo6O+PLLL1FaWoqysjL8+eefAIAXXngB33//PS5evAgAKCkpwZEjR2Qd19PTE3Xq1MHOnTuh0+lw5swZJCUlyX5ckEqlQkhICLZv346EhASUlZXh2rVrWL16NUpKShAcHCxte/36dfz888/Q6XQ4cuQI/vrrL3Ts2LHCMR/Wn06dOuHatWvYtWsXysvLcevWLWlq0t7eHvn5+Q+90tbPzw+HDx9GQkKCNOUJAM8//zz27NmDjIwMCCFQWlqKEydO4NatWw88VlBQEA4fPoyjR48aXeV569Yt2NjYwNLSEmfPnjWaQr5f06ZNcejQIeh0OmRmZhr9geXv74+kpCQkJyfDYDCgrKwMZ86cMfoDjf4+jvyI7uHr64vvvvsO6enpaN68uXTVna2tLaZPn47o6GisXbsWLi4umD59+kMfKNqvXz9ERUVhz5498Pf3R2hoKDp06ICJEyfCysoKwcHB0Gg0suoyMzPDtGnTsGHDBowZMwYqlQq+vr5o1aoVfHx8UFpaiiVLlkCr1cLa2hrt2rWTnvz9MGq1GlOnTsW6deuwY8cO1K9fH+PGjUPDhg3lvWEAevToAQsLC2zfvh1r1qyBWq3Gc889h08//dRo2tPT0xO5ubkIDw+Hg4MDJk+ebLT+rof1p27duvjwww8RExODbdu2Qa1WIzg4GJ6enujevTvi4+MRHh6OBg0aIDIyssKxPT09YWVlhcLCQqPgdXd3x8iRI7Fhwwbk5uZK51TvXolbmdatW8Pa2hoWFhbw8PCQ2ocPH45NmzZhw4YN8PLyQvfu3VFcXFzpMQYNGoSlS5ciLCwMXl5e8PX1lb4eotFoMHXqVGzZsgVLly6FmZkZPDw8MGLEiKo/FKoSn+dH9P9FRUXByckJgwcPru1SFCciIgK9evXi9+SoxnDak4hq1e3bt3HlypUKF8wQmRLDj4hqzfXr1/Huu+/Cy8tL+uoGUU3gtCcRESkOR35ERKQ4DD8iIlIchh8RESkOw4+IiBSH4UdERIrz/wCJkiJaTqAtdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var_smoothing': 0.23126489489556085}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(var_smoothing=0.23126489489556085)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=sklearn.naive_bayes.GaussianNB(var_smoothing=trial.params['var_smoothing'])\n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16545  2633]\n",
      " [  395 18783]]\n",
      "0.9210553759516112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92     19178\n",
      "         1.0       0.88      0.98      0.93     19178\n",
      "\n",
      "    accuracy                           0.92     38356\n",
      "   macro avg       0.93      0.92      0.92     38356\n",
      "weighted avg       0.93      0.92      0.92     38356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=nb.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:26:35,506]\u001b[0m A new study created in memory with name: no-name-77604ed0-e7d2-4dab-a4bd-6cf005678a3a\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:36,349]\u001b[0m Trial 0 finished with value: 0.9515868376397864 and parameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_split': 20, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9515868376397864.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:37,538]\u001b[0m Trial 1 finished with value: 0.9546263571242001 and parameters: {'criterion': 'entropy', 'max_depth': 11, 'min_samples_split': 19, 'min_samples_leaf': 15}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:38,415]\u001b[0m Trial 2 finished with value: 0.9502087542092367 and parameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 20}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:39,102]\u001b[0m Trial 3 finished with value: 0.9432796892182945 and parameters: {'criterion': 'gini', 'max_depth': 6, 'min_samples_split': 13, 'min_samples_leaf': 19}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:40,133]\u001b[0m Trial 4 finished with value: 0.9533641948152285 and parameters: {'criterion': 'gini', 'max_depth': 11, 'min_samples_split': 12, 'min_samples_leaf': 11}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:41,132]\u001b[0m Trial 5 finished with value: 0.9533384139975993 and parameters: {'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:41,720]\u001b[0m Trial 6 finished with value: 0.9313148449977655 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 16}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:42,754]\u001b[0m Trial 7 finished with value: 0.9533126789612245 and parameters: {'criterion': 'gini', 'max_depth': 11, 'min_samples_split': 18, 'min_samples_leaf': 12}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:43,769]\u001b[0m Trial 8 finished with value: 0.9515739472309718 and parameters: {'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 18}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n",
      "\u001b[32m[I 2022-08-06 20:26:44,705]\u001b[0m Trial 9 finished with value: 0.9491011444385008 and parameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 20}. Best is trial 1 with value: 0.9546263571242001.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    classifier_name = 'decision-tree'\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    if classifier_name == 'decision-tree':\n",
    "        criterion = trial.suggest_categorical('criterion', ['entropy','gini'])\n",
    "        max_depth = trial.suggest_int('max_depth', 5, X_train.shape[1])\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n",
    "        classifier_obj = sklearn.tree.DecisionTreeClassifier(criterion=criterion,max_depth=max_depth,min_samples_split=min_samples_split,\n",
    "                                                            min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    score = model_selection.cross_val_score(classifier_obj, X_train, y_train, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAEaCAYAAAChLgbSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVxU9f4/8NcMO7K4DEjIogEKKO47ImhovzSta7lk4gam4ZKZe5mKueXVrxuaoYiWuYZdzdQrboC4YlQCCgIuXFEYEVERh5n5/P7wOtcRlGFiE17Px6PHgznL57zPZ8gXn8+cOUcihBAgIiKiMpFWdQFERESvIwYoERGRHhigREREemCAEhER6YEBSkREpAcGKBERkR4YoERERHpggFKNN3LkSPj7+5e4TiKR4Mcff6zkimqnoKAg+Pn5Vegx5s2bB1dX1wo9RnkwNDREREREVZdBfxMDlKgaKCoqQkXe00ShUFRY21XhdT2f17VuKhkDlOi/RowYgd69exdb3qNHD4wcORLA/0Y4P/30E958802YmprC398fGRkZWvscOXIE3t7eMDMzQ6NGjTBq1CjcvXtXs/7ZqHjNmjVo3LgxTExM8OjRI/j5+WH06NGYOXMmZDIZrKysEBQUhMePH2u17efnh/r168Pa2hq+vr44d+6c1vElEglWr16NoUOHwtraGh9//DEA4Msvv4SHhwfMzc3h6OiIcePG4f79+5r9IiIiYGhoiOPHj8PLywtmZmbw9fXFrVu3EB0djTZt2qBOnTrw9/fHf/7zH53Ped68edi0aRNOnjwJiUQCiUSiGYE9fPgQn332GRo1agRzc3O0adMGkZGRmnavXbsGiUSCbdu2oU+fPqhTpw5mz56t03v67P3atWsX3NzcYG5ujvfffx/5+fmIjIxEs2bNYGlpiQ8//FCrH569PytWrNDU9cEHH0Aul2u2EULgn//8J958800YGxvDxcUFK1eu1Dp+48aN8dVXXyE4OBgNGjSAt7c3GjduDJVKhVGjRmn6AgDu3buHYcOGwcnJCWZmZmjWrBmWL1+u9YfVs7q+//57ODs7w8rKCu+99x5ycnK0jhsVFQUfHx+Ym5trfkfS0tI063fs2IHWrVvD1NQUjRs3xpQpU/Do0SPN+tjYWHh7e8PS0hKWlpZo1aoVDh8+rFOf1yqCqIYbMWKEeOutt0pcB0D88MMPQggh4uLihEQiEenp6Zr1V69eFRKJRMTGxgohhJg7d64wNzcX3t7e4ty5c+LcuXOiY8eOomXLlkKtVgshhDh69KgwMzMTq1evFikpKeLcuXPCz89P+Pj4aLYZMWKEsLS0FO+//774/fffxZ9//imKioqEr6+vsLS0FEFBQSIpKUns27dP2NjYiIkTJ2pqioyMFLt27RJXrlwRly5dEoGBgaJevXpCLpdrnVf9+vXF6tWrxdWrV8WVK1eEEEIsWLBAREdHi4yMDBEVFSWaNWsmhg8frtlv8+bNQiKRCF9fX3HmzBkRHx8vXF1dRbdu3YSvr684ffq0uHjxomjWrJkYNGiQZr/SzvnBgwdi6NChokuXLiIrK0tkZWWJgoICoVarhZ+fn/D19RUxMTEiLS1NbNiwQRgZGYmoqCghhBAZGRkCgGjUqJH44YcfRFpamtZ79Ly5c+cKFxcXrdfm5uaiT58+4o8//hAnTpwQMplM9OrVS7zzzjsiISFBREdHC1tbWzF9+nSt3xlLS0vRr18/8eeff4rjx48LV1dX0a9fP802a9euFaampmLDhg0iJSVFrF+/XpiYmIiNGzdqtnF2dhaWlpZi7ty54sqVKyIxMVFkZ2cLAwMDsXLlSk1fCCFEVlaWWLJkiYiPjxfp6enihx9+EHXq1BHh4eFadVlZWYkhQ4aIv/76S5w6dUo4OTlpvYdHjhwRUqlUfPbZZyIhIUEkJyeLjRs3iuTkZM17XLduXbF161aRlpYmTp48Kby8vMSwYcOEEEIolUpRr1498fnnn4uUlBSRkpIiIiMjRXR0dIl9XpsxQKnGGzFihDAwMBB16tQp9t/zASqEEF5eXuLLL7/UvJ45c6bw9PTUvJ47d64AIFJTUzXLrly5IgCII0eOCCGE8PX1FTNmzNCq4fr16wKA+P333zU1WVtbiwcPHmht5+vrK5ydnYVSqdQs27BhgzA2NhYPHz4s8fxUKpWoW7eu+PHHHzXLAIjRo0eX2jeRkZHC2NhYqFQqIcTTf1yfr1MIIb799lsBQFy4cEGzbMWKFaJBgwZadZd2zoGBgcLX11drm+PHjwsTExORl5entXzUqFHivffeE0L8L0BDQkJKPZ+SAtTAwEDk5ORolgUHBwupVCqys7M1yyZNmiTatWuneT1ixAhRp04drboOHz4sAIiUlBQhhBAODg5i2rRpWsefPHmyaNKkiea1s7Oz6NmzZ7E6DQwMxObNm0s9n0mTJgl/f3+tumQymSgsLNQsW7x4sbCzs9O87tatm+jbt+9L23R2dhbr16/XWnby5EkBQOTm5orc3FwBQBw/frzU+mo7TuFSrdCpUyckJCQU++9FY8eOxebNm6FSqaBUKhEREYExY8ZobWNjY6N1oUrTpk0hk8mQlJQEADh//jxWrlwJCwsLzX+enp4AgNTUVM1+Hh4esLCwKFZDx44dYWBgoHnt7e0NhUKhmYLLyMhAQEAAXF1dYWVlBSsrK9y/fx/Xr18v1s6LIiMj0b17d9jb28PCwgIff/wxFAoFbt++rdlGIpHAy8tL89rOzg4A0LJlS61ld+/ehUqlKtM5v+j8+fNQKBRo1KiR1r4//vhjsf1KOh9dNGrUCDKZTKt2Ozs72NjYaC3Lzs7W2s/T0xPW1taa197e3gCA5ORk5OfnIzMzE927d9fax9fXF9euXUNBQUGZ61ar1ViyZAlat24NmUwGCwsLfPfdd8XeVw8PD5iYmGid3507dzSv4+PjS/woAgBycnJw/fp1TJkyRau/33nnHQDA1atXUa9ePQQFBeHtt9/GO++8gyVLluDKlSs6nUNtY1jVBRBVBjMzM52uzgwICMCMGTNw4MABqNVq3Lt3D8OHDy91P/Hc51RqtRozZsxAQEBAse2ehREA1KlTR6faxQsXF7377ruQyWQIDQ2Fo6MjjI2N0a1bt2IXqLzY/tmzZzFw4EDMmjULy5YtQ7169XDmzBmMGDFCa1+pVKoV4M8+ozMyMiq27Fltup7zi9RqNaytrXH+/Pli64yNjV95Prp6vm7gae0lLVOr1WVu+1k/PPPiewXoXvfy5cuxePFirFixAm3btoWlpSX+7//+DwcOHNDa7sV+kUgkxY77Yl3PPDvHVatWoUePHsXWOzg4AADCwsLw2Wef4d///jeOHDmCOXPmYO3atRg7dqxO51JbMECJnmNlZYUhQ4YgLCwMarUaH3zwAerXr6+1TU5ODtLS0uDi4gIASElJwd27d+Hh4QEAaN++PRITE/X+OsX58+ehUqk0IXb69GnNRSp3795FUlISfvvtN7z99tsAgMzMzGKjp5LExsZCJpPhm2++0Szbs2ePXjW+SJdzNjY21oxYn98vLy8PhYWFaNGiRbnUUl6ejTStrKwAAHFxcQCejgCtrKzg4OCAkydPom/fvpp9oqOj0aRJE5ibm7+y7ZL6Ijo6Gv/v//0/BAYGapa9avT+Mu3atcPhw4cxceLEYusaNmwIR0dHXLlypdjMyotatGiBFi1aYMqUKRg3bhy+//57BugLOIVL9IKxY8fi4MGDOHz4MD755JNi683NzTFq1CjEx8fjwoULGDFiBLy8vDTfNQ0JCcG//vUvfP7550hISEBaWhoOHTqEwMBAratpX+bu3bsYP348kpOTceDAAcyZMwdjxoxBnTp1UK9ePdjY2CAsLAwpKSk4ffo0PvroI5iZmZXabrNmzZCTk4NNmzYhPT0dW7duxbp168reQSXQ5ZybNGmCy5cvIzExEXK5HE+ePEHPnj3h7++PAQMGYO/evUhPT0d8fDzWrFmDsLCwcqlNXxKJBMOHD8elS5cQHR2N8ePHo2/fvnBzcwMAzJo1S1NnamoqNmzYgPXr1+t0hXCTJk1w/Phx3Lp1S3Nlb7NmzXDixAkcP34cKSkp+Oqrr3D27Nky1z1nzhwcPHgQkydPxp9//okrV64gIiJCMw27cOFCrF69Gt988w0uXbqEK1eu4JdfftGE49WrVzFjxgzExsbi+vXrOH36NGJiYjRT8vQ/DFCiF3To0AFeXl5wcXGBr69vsfVvvPEGPvnkE3zwwQear23s3btXM23Wo0cPHDt2DH/99Rd8fHzQsmVLfP7557C0tCw2dViSDz/8EJaWlujWrRuGDBmCPn364NtvvwXwdHp19+7dSEtLQ8uWLTFy5EhMnjwZb7zxRqntvvvuu/jyyy8xe/ZseHl5YceOHVi2bFkZe6dkupxzYGAgOnTogK5du8LGxgbbt2+HRCLBvn37MGDAAEyZMgXu7u7o27cvDhw4oBnhV5WOHTuiW7du6NWrF95++200b94cmzdv1qz/9NNPERISgkWLFsHT0xNLly7FkiVLtEaQL7N8+XLEx8ejSZMmms9i58yZA19fX7z33nvo0qUL7t27h0mTJpW57t69e+O3337D2bNn0alTJ3Ts2BFbtmzRvA8BAQHYtWsXDhw4gI4dO6JDhw6YN28eGjVqBODplHNqaiqGDBmCpk2b4oMPPkDXrl2xdu3aMtdS00lESZP2RLWYUqmEs7MzpkyZgi+++EJr3bx58/Djjz/i6tWrFXJsPz8/uLq6YuPGjRXSPulm5MiRyMzMRFRUVFWXQtUYPwMl+i+1Wo3s7Gxs2LABDx8+RFBQUFWXRETVGAOU6L9u3LiBJk2a4I033sDmzZu1vsJARPQiTuESERHpgRcRERER6YEBSkREpAd+BlrL3Lp1q6pLqHZkMpnWUzboKfZLydgvJavJ/WJvb1/ico5AiYiI9MAAJSIi0gMDlIiISA8MUCIiIj0wQImIiPTAACUiItIDA5SIiEgPDFAiIiI98EYKtcy7my5XdQlERJXq10D3CmmXI1AiIiI9MECJiIj0wAAlIiLSAwOUiIhIDwxQIiIiPTBAiYiI9MAAJSIi0gMDlIiISA8MUCIiIj0wQImIiPTAACUiItIDA5SIiEgPDFAiIiI9MECJiIj0wAAlIiLSAwOUiIhIDwxQIiIiPTBAiYiI9MAArUTjx49Hfn6+XvueOHECubm55dIWERH9fQzQ18SJEydw7969qi6DiIj+y7CqC6gK2dnZWLRoEdzd3ZGamgpnZ2f4+flh9+7duH//PiZNmgQAiIiIgEKhgLGxMYKDg2Fvb49ff/0VN27cQHBwMG7cuIFVq1Zh0aJFMDExKXacBw8eYNWqVcjPz4erqyuEEJp10dHROHjwIJRKJdzc3BAUFASpVIqAgAD06tULiYmJqFOnDiZPnoykpCSkpaVh9erVMDY2xsKFCwEAhw4dQnx8PJRKJaZMmYJGjRoVqyEqKgpRUVEAgCVLllREdxIRVWsymaxC2q2VAQoAt2/fxpQpU+Dg4IBZs2YhNjYWISEhuHDhAiIjIzFhwgTMnz8fBgYG+PPPP/HTTz9h6tSp6NOnD+bPn49z584hMjISY8aMKTE8AWD37t1wd3fHhx9+iIsXL2qCLDMzE3FxcViwYAEMDQ2xceNGxMTEwNfXF0+ePEGTJk0wfPhw7NmzB7t370ZgYCAOHTqEgIAAuLi4aNq3tLTE0qVLcfjwYezfvx/jxo0rVoO/vz/8/f0rphOJiF4Dcrn8b+1vb29f4vJaG6C2trZwcnICADg6OsLLywsSiQROTk7IyclBQUEBQkNDcfv2bQCASqUCAEilUgQHB2Pq1Kno1asX3N3dX3qM5ORkTJ06FQDQtm1b1KlTBwBw6dIlZGRkYNasWQAAhUIBKysrAIBEIkHXrl0BAD4+PvjnP//50vY7deoEAHjzzTdx7tw5vfuCiIjKrtYGqJGRkeZniUSieS2RSKBWq7Fz5040b94c06ZNQ3Z2NubPn6/ZPisrC6amploX9byMRCIptkwIAV9fXwwdOlSv/Z8xNHz69kmlUk3AExFR5eBFRC9RUFCA+vXrA3h6Ac/zyyMiIjB//nw8fPgQZ86ceWkbHh4eiImJAQD8/vvvePToEQDAy8sLZ86cwf379wEADx8+RE5ODoCn4fqszdjYWM0I19TUFI8fPy7fkyQiIr0xQF/ivffew/bt2zFnzhyo1WrN8oiICPTu3Rv29vYYN24ctm3bpgnCFw0cOBDJycmYMWMG/vjjD80H2Q4ODhgyZAi++eYbTJ06FQsWLNBcYWtiYoKbN29ixowZuHTpEj788EMAgJ+fH8LCwjBt2jQoFIoKPnsiIiqNRDx/aShVuYCAAPzwww8V1n7bBccqrG0iouro18CXX6uii5ddRMQRKBERkR5q7UVE5en48eP47bfftJY1a9YMQUFBZW6rIkefRERUfhig5aBHjx7o0aNHVZdBRESViFO4REREemCAEhER6YEBSkREpAcGKBERkR4YoERERHpggBIREemBAUpERKQHBigREZEeGKBERER6YIASERHpgQFKRESkBz7OrJa5detWVZdQ7chkMsjl8qouo9phv5SM/VKymtwvfJwZERFROWKAEhER6YEBSkREpAcGKBERkR4YoERERHpggBIREemBAUpERKQHBigREZEeGKBERER6MNR1Q7VaDamUefu6e3fT5TLv82ugewVUQkT0etMpEdVqNQICAlBUVFTR9RAREb0WdApQqVQKe3t7PHjwoKLrISIiei3oPIXbrVs3LF26FO+88w4aNGgAiUSiWdeiRYsKKY6IiKi60jlA//3vfwMAdu/erbVcIpFg7dq15VsVERFRNadzgIaGhlZkHURERK+VMl1Wq1QqkZycjLi4OABAYWEhCgsLK6QwIiKi6kznEeiNGzewdOlSGBkZ4e7du+jatSuSkpJw8uRJfP755xVZIxERUbWj8wg0LCwMgwcPxsqVK2Fo+DR3PT09cfly2b9XSERE9LrTOUAzMzPh4+OjtczU1BQKhaLciyIiIqrudA5QGxsbpKenay27evUq7Ozsyr0oIiKi6k7nz0AHDx6MJUuWoFevXlAqldi7dy+OHDmCsWPHVmR9RERE1ZLOI9B27dph1qxZyM/Ph6enJ3JycjB16lS0atWqIusjIiKqlnQegZ4+fRpdunTBm2++qbX8zJkz6Ny5c7kXRkREVJ3pPAL97rvvSly+YcOGciuGiIjodVHqCPTOnTsAnj6RJTs7G0IIrXXGxsYVVx0REVE1VWqATpo0SfPzxIkTtdbVrVsXAwcOLP+qiIiIqrlSA3Tnzp0AgLlz52L+/PkVXhAREdHrQOfPQJ+Fp1wuR0pKSoUVRERE9DrQOUDlcjnmzJmDzz//HAsWLADw9Arcl11cpK8LFy7gl19+Kdc2K8L48eORn59fLm2FhobizJkzeu2bn5+P2bNnY/r06UhOTi6XeoiIqHQ6B+j333+PNm3aYMuWLZp74bZs2RJ//vlnuRbUvn17vP/+++XaZk32119/wd7eHt9++y08PDyquhwiolpD5++BXr16FTNnzoRU+r/MNTc3R0FBgc4Hy87OxqJFi+Du7o7U1FQ4OzvDz88Pu3fvxv379zFp0iRkZmYiLS0NgYGBCA0NhZmZGdLT05GXl4dhw4a99Dun9+7dw8qVK1FQUAC1Wo2goCB4eHggLCwMaWlpUCgU6Ny5MwYNGgTg6QjS29sbiYmJUKlU+OSTT7B9+3bcvn0b/fr1Q+/evZGYmIhdu3bBwsICt27dgoeHB4KCgrT6AACio6Nx8OBBKJVKuLm5ISgoCACwfv16ze0Pe/TogXfffbfUPkpPT8eWLVtQWFgIKysrBAcHo169eoiKisLRo0ehVCrRsGFDTJw4EVlZWfjxxx+hUCgwbdo0LFy4sNhV0VFRUYiKigIALFmyROf36nkymUyv/V4XhoaGNf4c9cF+KRn7pWS1sV90DlBra2vcvn0b9vb2mmWZmZll7rDbt29jypQpcHBwwKxZsxAbG4uQkBBcuHABkZGR6Nixo9b2eXl5CAkJwa1bt7B06dKXBmhsbCxatWqFAQMGQK1W48mTJwCAjz76CBYWFlCr1QgJCcH169fh7OwM4GkwLFy4EBEREVi3bh0WLFiAoqIiTJkyBb179wbw9A+HFStWwMbGBgsXLsS5c+e0asjMzERcXBwWLFgAQ0NDbNy4ETExMXB0dERubi6WL18OAHj06FGpfaNUKhEeHo7p06fDysoKcXFx2L59O4KDg9GpUyf4+/sDAHbs2IFjx47hnXfeweDBgzV/cJTE399fs5++5HL539q/upPJZDX+HPXBfikZ+6VkNblfns+95+kcoP369cPSpUvx/vvvQ61WIzY2Fnv37i3zdKutrS2cnJwAAI6OjvDy8oJEIoGTkxNycnKKbd+hQwdIpVI4ODjg/v37L23XxcUF69evh1KpRMeOHdG4cWMAQFxcHI4ePQqVSoV79+4hMzNTE6Dt27cHADg5OaGwsBBmZmYwMzODkZGRJvBcXV3RsGFDAIC3tzcuX76sFaCXLl1CRkYGZs2aBQBQKBSwsrJCu3btkJ2djfDwcLRt2xYtW7YstW9u3bqFmzdvaj5jVqvVqFevHgDg5s2b2LFjBx49eoTCwkLeQpGIqIrpHKA9e/aEhYUFjh49igYNGuDkyZMYPHhwsRFjaYyMjDQ/SyQSzWuJRAK1Wv3K7Z+/icOLPD09MX/+fFy8eBFr1qxB//794eHhgf3792Px4sWwsLBAaGgoioqKNPs8+yxXKpVqHUcqlUKlUul0PkII+Pr6YujQocXWLVu2DAkJCTh06BDi4uIQHBxcansODg5YuHBhseWhoaGYNm0aGjdujBMnTiAxMVGn+oiIqGLoHKAA0LFjxzIHZmXJyclB/fr14e/vjydPniAjIwPOzs4wNTWFubk58vLykJCQgObNm5ep3atXryI7OxsymQynT5/GW2+9pbXey8sL3377Lfr27Qtra2s8fPgQjx8/homJCQwNDdG5c2fY2dkhNDS01GPZ29sjPz8fKSkpaNq0KZRKJbKysuDo6IjCwkLUq1cPSqUSMTExqF+/fpnOg4iIyleZAjQ5ORkZGRkoLCzUWj5gwIByLUofiYmJ2L9/PwwMDGBqaooJEybA1tYWjRs3xhdffAFbW1s0a9aszO02bdoU27Ztw40bN+Dh4VHsDwgHBwcMGTIE33zzDYQQMDAwQGBgIIyNjbF+/XrNqLqkEeqLDA0N8cUXX2Dz5s0oKCiASqVCnz594OjoiMGDB2P27NmwsbGBk5MTHj9+XOZzISKi8iMRr5oXfU54eDhOnz4Nd3d3rSs9JRIJJkyYUGEFVqVnoTxz5syqLqXctF1wrMz7/BroXgGVVB81+eKHv4P9UjL2S8lqcr/87YuIYmJisHz5ck4dEhERoQwBKpPJtC60qSo3btzAmjVrtJYZGRlh0aJF5X6s5s2bl/kz01fZuHEjrly5orWsT58+6NGjR7kdg4iIKofOATpu3Dhs2LAB3t7esLa21lrn6elZ7oW9jJOTE5YtW1ZpxytPz26wQERErz+dAzQ9PR2///47kpOTi93tZv369eVeGBERUXWmc4Bu374dM2bM0OmGAERERDWdzjeTNzExqdSpWiIioupM5wAdPHgwIiIikJeXB7VarfUfERFRbaPzFO6zzzmPHDlSbN3OnTvLryIiIqLXgM4Bunbt2oqsg4iI6LWic4Da2NhUZB1ERESvlTLdC/fChQtISkpCfn6+1vKaeis/IiKil9H5IqLdu3fj+++/h1qtxpkzZ2BhYYE//vgD5ubmFVkfERFRtaTzCPT48eP46quv4OTkhBMnTmDkyJHo1q0bfv7554qsj4iIqFrSeQT66NEjODk5AXj62C2lUglXV1ckJSVVWHFERETVlc4jUDs7O9y8eROOjo5wdHTEv//9b1hYWMDCwqIi66NyVtMfTUZEVFl0DtDBgwfjwYMHAICPP/4Yq1atQmFhIW+QTkREtZJOAapWq2FsbIymTZsCAFxdXYs9UoyIiKg20ekzUKlUim+//RaGhmX61gsREVGNpfNFRB4eHkhJSanIWoiIiF4bZboT0eLFi9G+fXs0aNAAEolEs27w4MEVUhwREVF1pXOAKhQKdOjQAQCQm5tbYQURERG9DnQO0ODg4Iqsg4iI6LVS5quCHj9+jAcPHkAIoVnWsGHDci2KiIioutM5QDMzM7F69Wpcv3692Do+D5SIiGobnQN048aNaN68OebOnYsJEyYgNDQUP/30k+a7ofR6eHfT5Veu552KiIh0o/PXWK5fv46PP/4YderUgRAC5ubmGDZsGEefRERUK+kcoEZGRlCpVAAAS0tLyOVyCCHw8OHDCiuOiIioutJ5Ctfd3R2nT5+Gn58fOnfujEWLFsHIyAjNmzevyPqIiIiqJZ0DdMqUKZqfP/roIzg6OqKwsBDdu3evkMKIiIiqszJ/jeXZtK2Pj4/W3YiIiIhqE50D9NGjRwgPD8eZM2egVCphaGiIzp07Y9SoUXwmKBER1To6X0S0bt06KBQKLF26FFu3bsXSpUtRVFSEdevWVWR9RERE1ZLOAZqYmIiJEyfCwcEBJiYmcHBwwPjx45GUlFSR9REREVVLOgeovb09srOztZbJ5XLY29uXe1FERETVnc6fgbZo0QILFy6Ej48PZDIZ5HI5YmJi0L17dxw7dkyzXc+ePSukUCIioupE5wBNTU2FnZ0dUlNTkZqaCgCws7NDSkqK1oO2GaBERFQb6BSgQgiMGzcOMpkMBgYGFV0TERFRtafTZ6ASiQRTp07l9z6JiIj+S+eLiBo3boysrKyKrIWIiOi1ofNnoM2bN8eiRYvg6+sLmUymtY6fexIRUW2jc4BeuXIFtra2SE5OLraOAUpERLWNzgE6d+7ciqyDiIjotaLzZ6AA8ODBA0RHR2Pfvn0AgNzcXNy9e7dCCiMiIqrOdA7QpKQkTJ48GTExMdizZw8A4Pbt2wgLCyuXQi5cuIBffvmlXNqqSOPHj0d+fnl/XHkAAB0DSURBVH6lHzc7OxtffPEFACAtLQ3h4eEAnt5i8cqVK5VeDxFRbafzFG5ERAQmT54MLy8vjBo1CgDg6uqKtLS0cimkffv2aN++fbm0VdO5uLjAxcUFwNMANTU1RbNmzaq4KiKi2kXnAM3JyYGXl5f2zoaGUKlUpe6bnZ2NRYsWwd3dHampqXB2doafnx92796N+/fvY9KkScjMzERaWhoCAwMRGhoKMzMzpKenIy8vD8OGDUPnzp1LbPvevXtYuXIlCgoKoFarERQUBA8PD4SFhSEtLQ0KhQKdO3fGoEGDADwdQXp7eyMxMREqlQqffPIJtm/fjtu3b6Nfv37o3bs3EhMTsWvXLlhYWODWrVvw8PBAUFAQpFLtAXt0dDQOHjwIpVIJNzc3BAUFAQDWr1+P9PR0AECPHj3w7rvvllj7b7/9hiNHjsDAwAAODg6YPHkydu3ahTt37mimx/v37w9/f3+t/RITE7F//36MHj0aR44cgVQqRUxMDEaPHg0PDw+tbaOiohAVFQUAWLJkSanv1YtXWNcGhoaGtfK8S8N+KRn7pWS1sV90DlAHBwckJCSgdevWmmV//fUXnJycdNr/9u3bmDJlChwcHDBr1izExsYiJCQEFy5cQGRkJDp27Ki1fV5eHkJCQnDr1i0sXbr0pQEaGxuLVq1aYcCAAVCr1Xjy5AkA4KOPPoKFhQXUajVCQkJw/fp1ODs7A3gaEgsXLkRERATWrVuHBQsWoKioCFOmTEHv3r0BAFevXsWKFStgY2ODhQsX4ty5c1o1ZGZmIi4uDgsWLIChoSE2btyImJgYODo6Ijc3F8uXLwfw9DmqL/Ovf/0La9euhZGRkdZ2N27cwMKFC1FYWIgZM2agbdu2Je5va2uLXr16wdTUFP379y9xG39//2IB/CpyuVznbWuKZ/d2Jm3sl5KxX0pWk/vlZQ9N0TlAAwICsHTpUrRp0wYKhQLff/894uPjMW3aNJ32t7W11YSto6MjvLy8IJFI4OTkhJycnGLbd+jQAVKpFA4ODrh///5L23VxccH69euhVCrRsWNHNG7cGAAQFxeHo0ePQqVS4d69e8jMzNQE6LOpYicnJxQWFsLMzAxmZmZaQebq6oqGDRsCALy9vXH58mWtAL106RIyMjIwa9YsAIBCoYCVlRXatWuH7OxshIeHo23btmjZsuVLa3dycsLq1avRoUMHrT8g2rdvD2NjYxgbG6N58+a4evWq5ryIiKh60DlAmzZtimXLliEmJgampqaQyWRYtGgRGjRooNP+RkZGmp8lEonmtUQigVqtfuX2QoiXtuvp6Yn58+fj4sWLWLNmDfr37w8PDw/s378fixcvhoWFBUJDQ1FUVKTZx9Dw6WlLpVKt40ilUp2mpJ/V5Ovri6FDhxZbt2zZMiQkJODQoUOIi4tDcHBwiW3MmjULSUlJuHDhAn7++WesWLECAIrdMpG3UCQiqn7K9DWW+vXro3///hg0aBDee+89ncOzIuXk5MDa2hr+/v7o2bMnMjIyUFBQAFNTU5ibmyMvLw8JCQllbvfq1avIzs6GWq3G6dOn4e7urrXey8sLZ86c0YyOHz58iJycHOTn50OtVqNz584YMmQIMjIySmxfrVZDLpejRYsWGDZsGAoKClBYWAgAOH/+PBQKBR48eIDExETNBUMlMTMz0+xHRESVR+cR6KNHjxAeHo4zZ85AqVTC0NAQnTt3xqhRo2BhYVGRNb7SswtqDAwMYGpqigkTJsDW1haNGzfGF198AVtbW72uUG3atCm2bduGGzduwMPDo9hntA4ODhgyZAi++eYbCCFgYGCAwMBAGBsbY/369ZpRdUkjVOBpgK5ZswYFBQUAgL59+6JOnToAnk4fL1myBHK5HB988AHq169f7GHmz7Rr1w4rVqzA+fPnS7yIiIiIKoZEvGp+9DnLli2DVCrF4MGDYWNjg5ycHOzatQtKpRLTp0+v6Dor1bNQnjlzZqUfe9euXa+8KOjvarvg2CvX/xro/sr1NVFNvvjh72C/lIz9UrKa3C8vu4hI5yncxMRETJw4EQ4ODjAxMYGDgwPGjx+PpKSkciuSiIjodaHzFK69vT2ys7Ph4OCgWSaXy1+azOXtxo0bWLNmjdYyIyMjLFq0qNyP1bx5czRv3rzc2tu4cWOxuwX16dMHPXr0KLbts++rEhFR9aZzgLZo0QILFy6Ej4+PZqgeExOD7t2749ix/00LVtSTWZycnLBs2bIKabuiPbvBAhER1Rw6B2hqairs7OyQmpqK1NRUAICdnR1SUlKQkpKi2Y6PNiMiotqAjzMjIiLSg84XEW3ZsgXXrl2rwFKIiIheHzqPQFUqFRYuXAgrKyv4+PjAx8enWtxIgYiIqCroHKCjR4/GyJEj8fvvvyMmJgaRkZFwc3ND9+7d0alTJ5iamlZknURERNWKzgEKPL1XbLt27dCuXTvcvHkTq1evxrp167Bx40Z4e3tj0KBBqF+/fkXVSkREVG2UKUALCgpw5swZxMTE4Pr16+jUqRMCAwMhk8nw66+/YtGiRfjnP/9ZUbUSERFVGzoH6PLly5GQkABPT0/06tULHTp00HqSyfDhwzFy5MiKqJGIiKja0TlA3dzcEBgYiLp165a4XiqVIiwsrNwKIyIiqs5KDdCvv/5a8zzK+Pj4EreZP38+AMDExKQcSyMiIqq+Sg3QF+8stGnTJgQGBlZYQURERK+DUgPUz89P6/WWLVuKLaPXR218XBkRUUXQ+U5ERERE9D8MUCIiIj2UOoV76dIlrddqtbrYshYtWpRvVURERNVcqQG6fv16rdcWFhZayyQSCdauXVv+lREREVVjpQZoaGhoZdRBRET0WuFnoERERHpggBIREemBAUpERKQHBigREZEeyvQ4M3r9vbvp8kvX8S5FRES64wiUiIhIDwxQIiIiPTBAiYiI9MAAJSIi0gMDlIiISA8MUCIiIj0wQImIiPTAACUiItIDA5SIiEgPDFAiIiI9MECJiIj0wAAlIiLSAwOUiIhIDwxQIiIiPTBAiYiI9MAAJSIi0gMDlIiISA8M0HJ04cIF/PLLLwCAc+fOITMz82+1QURE1ZdhVRdQU6hUKrRv3x7t27cHAJw/fx7t2rWDg4OD3m0QEVH1xQAtg5MnT2L//v2QSCRwcnKCVCqFhYUFrl27hiZNmsDJyQlpaWno1q0bLly4gKSkJPz888/44osvAACbNm1Cfn4+TExMMHbsWDRq1AihoaElthEYGIicnBysX78e+fn5sLKyQnBwMGQyGUJDQ2FmZob09HTk5eVh2LBh6Ny5cxX3DhFR7cIA1dHNmzcRGRmJBQsWwMrKCg8fPsSWLVuQlZWFOXPmQCqV4sSJEwCAZs2aoX379mjXrp0m2EJCQjBmzBi88cYbSE1NxcaNGzF37lwAKLEN4Gngdu/eHX5+fjh27BjCw8Mxffp0AEBeXh5CQkJw69YtLF269KUBGhUVhaioKADAkiVLXnmOMpns73TRa8vQ0LDWnvursF9Kxn4pWW3sFwaoji5duoTOnTvDysoKAGBhYQEA6Ny5M6TSV3+UXFhYiCtXrmDFihWaZUqlUvPzy9pITU3F1KlTAQDdu3fHtm3bNOs6dOgAqVQKBwcH3L9//6XH9vf3h7+/vw5nCMjlcp22q2lkMlmtPfdXYb+UjP1SsprcL/b29iUuZ4DqSAgBiURSbLmpqWmp+6rVatSpUwfLli0rcb0ubbzIyMhIqzYiIqpcvApXR15eXjh9+jQePHgAAHj48OErtzczM8Pjx48BAObm5rC1tcXp06cBPA28a9eulXrMpk2bIi4uDgAQGxsLd3f3v3EGRERUnjgC1ZGjoyP+8Y9/YN68eZBKpWjcuPErt+/atSs2bNiAgwcPYsqUKZg0aRLCwsIQGRkJpVIJb2/vUtsYNWoU1q9fj3379mkuIiIioupBIjj/V6u0XXDspet+DaydI9ya/NnN38F+KRn7pWQ1uV9e9hkop3CJiIj0wAAlIiLSAwOUiIhIDwxQIiIiPTBAiYiI9MAAJSIi0gMDlIiISA8MUCIiIj0wQImIiPTAACUiItIDA5SIiEgPDFAiIiI9MECJiIj0wAAlIiLSAwOUiIhID3ygdi1TW5/5SURU3jgCJSIi0gMDlIiISA8MUCIiIj0wQImIiPTAACUiItIDA5SIiEgPDFAiIiI9MECJiIj0wAAlIiLSg0QIIaq6CCIiotcNR6C1yMyZM6u6hGqJ/VIy9kvJ2C8lq439wgAlIiLSAwOUiIhIDwzQWsTf37+qS6iW2C8lY7+UjP1SstrYL7yIiIiISA8cgRIREemBAUpERKQHw6ougMpXQkICNm/eDLVajbfeegvvv/++1vqioiKsXbsW6enpsLS0xOTJk2Fra1tF1Vae0vrl119/xdGjR2FgYAArKyt8+umnsLGxqaJqK1dpffPMmTNnsGLFCixevBguLi6VXGXl0qVP4uLisHv3bkgkEjg7O+Ozzz6rgkorX2l9I5fLERoaikePHkGtVmPo0KFo27ZtFVVbwQTVGCqVSkyYMEHcvn1bFBUVialTp4qbN29qbXPo0CGxYcMGIYQQsbGxYsWKFVVRaqXSpV/++usvUVhYKIQQ4vDhw7WiX4TQrW+EEKKgoEB8/fXXYvbs2eLq1atVUGnl0aVPbt26JaZNmyYePHgghBAiLy+vKkqtdLr0zXfffScOHz4shBDi5s2bIjg4uCpKrRScwq1Brl69Cjs7OzRs2BCGhobo2rUrzp8/r7XNhQsX4OfnBwDo3LkzLl26BFHDryPTpV9atGgBExMTAICbmxtyc3OrotRKp0vfAMDOnTvRv39/GBkZVUGVlUuXPjl69CjefvttWFhYAACsra2rotRKp0vfSCQSFBQUAAAKCgpQr169qii1UjBAa5Dc3Fw0aNBA87pBgwbFguD5bQwMDGBubo4HDx5Uap2VTZd+ed6xY8fQunXryiityunSNxkZGZDL5WjXrl1ll1cldOmTW7duISsrC3PmzMGXX36JhISEyi6zSujSNwMHDkRMTAzGjRuHxYsXY/To0ZVdZqVhgNYgJY0kJRJJmbepacpyztHR0UhPT0f//v0ruqxqobS+UavV2LJlC4YPH16ZZVUpXX5f1Go1srKyMHfuXHz22Wf47rvv8OjRo8oqscro0jenTp2Cn58fvvvuO8yaNQtr1qyBWq2urBIrFQO0BmnQoAHu3r2reX337t1i0yfPb6NSqVBQUKCZhqqpdOkXAPjzzz+xd+9eTJ8+vVZMVQKl901hYSFu3ryJ+fPnY/z48UhNTcW3336LtLS0qii3Uujy+1K/fn106NABhoaGsLW1hb29PbKysiq71EqnS98cO3YMXbp0AQA0bdoURUVFNXaWiwFag7i4uCArKwvZ2dlQKpWIi4tD+/bttbZp164dTpw4AeDpVZXNmzev8SNQXfolIyMDYWFhmD59eq35PAsovW/Mzc2xadMmhIaGIjQ0FG5ubpg+fXqNvgpXl9+Xjh074tKlSwCA/Px8ZGVloWHDhlVRbqXSpW9kMpmmbzIzM1FUVAQrK6uqKLfC8U5ENczFixexZcsWqNVq9OjRAwMGDMDOnTvh4uKC9u3bQ6FQYO3atcjIyICFhQUmT55cK/7HL61fFixYgBs3bqBu3boAnv4jMGPGjCquunKU1jfPmzdvHgICAmp0gAKl94kQAlu3bkVCQgKkUikGDBgAb2/vqi67UpTWN5mZmdiwYQMKCwsBAMOGDUOrVq2quOqKwQAlIiLSA6dwiYiI9MAAJSIi0gMDlIiISA8MUCIiIj0wQImIiPTAACUivZw7dw6ffvopAgICkJGRUSnHPHHiBObMmfPS9YsWLdJ8z7k8VVS7+srOzsagQYOgUqmqupRajY8zIyrB+PHjMXbsWLRs2bKqS8G8efPg4+ODt956q6pL0fLDDz9g9OjR6NChQ7m1GR8fjz179iAzMxNGRkZo3bo1Pv74Y637r77K7Nmz/3YNu3btwu3btzFp0qRybfdFkydPRv/+/dGzZ0+t5b/99huio6OxZMmScj8mlS+OQImqKSFEtb6HaE5ODhwdHfXat6TzOnPmDFavXo0+ffpg06ZNWLFiBQwNDfH111/j4cOHf7fcasfX1xfR0dHFlkdHR8PX17cKKqKy4giUqBQnTpzA0aNH4eLighMnTsDCwgITJ05EVlYWdu7ciaKiIgwbNkzzmLjQ0FAYGRnhzp07SE1NRZMmTTBhwgTNA7qvXLmCiIgI3Lp1C/b29hg5ciSaNWsG4Olos1mzZkhKSkJ6ejo6deqE5ORkpKamIiIiAn5+fggMDMTmzZtx7tw5FBQUwM7ODiNHjoSHhweApyOozMxMGBsb49y5c5DJZBg/frzm7kFyuRwRERFITk6GEALe3t4IDAwE8PQ+pvv370deXh5cXV3xySefFHuweFFREUaPHg21Wo1p06ahbt26WLNmDTIzM7Fx40Zcu3YN9evXx9ChQzV3MgoNDYWxsTHkcjmSkpIwbdo0rdH9szv7DBgwAD4+PgAAY2NjjBs3DtOmTcOBAwcwePBgzfbh4eE4efIk6tWrh8DAQHh5eWn67/nR+qvO5+bNm4iIiEB6ejoMDQ3xzjvv4M0338TevXsBAOfPn4ednR2WLVumabd79+4YM2YMQkJC4OTkBODprfw+/fRTrFu3DtbW1oiPj8eOHTuQk5MDBwcHjBkzBs7OzsV+r7p3746dO3ciJydHU1NmZiauX78Ob29vXLx4ETt27MCdO3dgbm6OHj16YNCgQSX+jr44Y/LiKDolJQVbt25FZmYmbGxsMHLkSDRv3rzkX3jSXRU8g5So2gsODhZ//PGHEEKI48ePi8GDB4tjx44JlUoltm/fLsaNGyfCwsKEQqEQCQkJIiAgQDx+/FgIIcTatWtFQECASExMFAqFQoSHh4uvvvpKCCHEgwcPxMiRI8XJkyeFUqkUMTExYuTIkSI/P18IIcTcuXPFuHHjxI0bN4RSqRRFRUVi7ty5IioqSqu+kydPivz8fKFUKsW+fftEUFCQePLkiRBCiJ07d4qhQ4eK+Ph4oVKpxLZt28Ts2bOFEE8fiDx16lSxefNm8fjxY/HkyRORnJwshBDi7NmzYsKECeLmzZtCqVSKPXv2iC+//PKlfTRw4ECRlZUlhBCiqKhITJgwQfz888+iqKhI/PXXXyIgIED85z//0fTJ8OHDRXJyslCpVJpan8nMzBQDBw4Ud+7cKXacnTt3aup/9l7s379fFBUViVOnTonhw4drHmz9fF+96nwKCgrEmDFjxL59+8STJ09EQUGBSElJ0Rxv1apVWjU8325oaKj46aefNOsOHjwovvnmGyGEEGlpaSIwMFCkpKQIlUoljh8/LoKDg4VCoSixD0NCQsSePXs0r7dt2yaWLl0qhBDi0qVL4vr160KlUolr166JoKAgcfbsWSGEEHfu3BEDBw4USqVSCKH9+/riOdy9e1eMGjVK8/vwxx9/iFGjRon79++XWBPpjlO4RDqwtbVFjx49IJVK0bVrV9y9excffvghjIyM0KpVKxgaGuL27dua7du2bQtPT08YGRnho48+QkpKCuRyOS5evAg7Ozt0794dBgYG6NatG+zt7REfH6/Z18/PD46OjjAwMIChYcmTRN27d4elpSUMDAzQr18/KJVK3Lp1S7Pe3d0dbdu2hVQqRffu3XHt2jUATx+InJubi4CAAJiamsLY2Bju7u4AgKioKPzjH/+Ag4MDDAwM8I9//APXrl1DTk5Oqf2TmpqKwsJCvP/++zA0NESLFi3Qtm1bxMbGarbp0KED3N3dIZVKYWxsrLX/s6d1PLsX8fPq1q2r9TQPa2tr9O3bV/NAZ3t7e1y8eLHYfq86n/j4eNStWxf9+vWDsbExzMzM4ObmVup5AkC3bt1w6tQpzetTp06hW7duAJ4+aNvf3x9ubm6QSqXw8/ODoaEhUlNTS2zr+WlctVqNmJgYzUxG8+bN4eTkBKlUCmdnZ3h7eyMpKUmnGp8XHR2NNm3aaH4fWrZsCRcXlxL7jMqGU7hEOnj+CS3P/vF//h97Y2Njzc2zAWhd9GJqagoLCwvcu3cPubm5xaZEbWxstB5KrMsFM/v378exY8eQm5sLiUSCx48fFwuZ52srKiqCSqWCXC6HjY0NDAwMirWZk5ODzZs3Y+vWrZplQogSa37RvXv3IJPJIJX+72/yspyXpaUlACAvLw+2trZa6/Ly8jTrgaePEnv+CUIvHkeX87l7967eD1Fo0aIFFAoFUlNTUbduXVy7dg0dO3YE8HR6/OTJkzh06JBme6VS+dIHuHfq1AmbNm1CSkoKFAoFFAoF2rZtC+DpHyU//fQTbty4AaVSCaVSic6dO5e5XrlcjjNnzmj9kaZSqTiFWw4YoEQV4PlnJhYWFuLhw4eoV68e6tevj7Nnz2ptK5fL0bp1a83rFx8v9+Lr5ORk/Otf/8LXX38NBwcHSKVSjBo1qsSHHb9IJpNBLpdDpVIVC1GZTKb1GWRZ1KtXD3K5HGq1WhOicrkcb7zxxkvP43n29vZo0KABTp8+jffee0+zXK1W4+zZs1pX+ubm5kIIoWlPLpcXe2pMaeeTk5OjNYp8XmmP95NKpejSpQtOnToFa2trtG3bFmZmZgCe/pEwYMAADBgw4JVtPGNiYoJOnTohOjoaCoUCXbt21cw6rF69Gm+//TZmzZoFY2NjREREID8//6XtKBQKzeu8vDzNzw0aNICPjw/GjRunU02kO07hElWA33//HZcvX4ZSqcSOHTvg5uYGmUyGNm3aICsrC7GxsVCpVIiLi0NmZqZm1FESa2tr3LlzR/P68ePHMDAwgJWVFdRqNfbs2YOCggKd6nJ1dUW9evWwbds2FBYWQqFQ4PLlywCAXr164ZdffsHNmzcBAAUFBTh9+rRO7bq5ucHU1BT79u2DUqlEYmIi4uPjdX7El0QiQUBAACIjIxEbGwuFQoG8vDx89913KCgoQN++fTXb3r9/HwcPHoRSqcTp06fxn//8B23atCnW5qvOp127dsjLy8OBAwdQVFSEx48fa6ZZra2tkZOT88oroLt164a4uDjExsZqpm8B4K233sKRI0eQmpoKIQQKCwtx8eJFPH78+KVt+fn5IS4uDmfPntW6+vbx48ewsLCAsbExrl69qjUd/qLGjRvj1KlTUCqVSEtL0/ojzcfHB/Hx8UhISIBarYZCoUBiYqLWH3mkH45AiSqAt7c3du/ejZSUFLz55puaqyEtLS0xc+ZMbN68GWFhYbCzs8PMmTNf+cDhPn36IDQ0FEeOHIGPjw9GjhyJ1q1b47PPPoOJiQn69u0LmUymU11SqRQzZsxAeHg4goODIZFI4O3tDXd3d3Ts2BGFhYVYuXIl5HI5zM3N4eXlhS5dupTarqGhIaZPn46NGzdi7969qF+/PiZMmIBGjRrp1mEAunbtCiMjI0RGRmLDhg0wNDREq1atsGDBAq0pXDc3N2RlZSEwMBB169bFlClTtNY/86rzMTMzw1dffYWIiAjs2bMHhoaG6Nu3L9zc3NClSxfExMQgMDAQtra2WLp0abG23dzcYGJigtzcXK3wdnFxwdixYxEeHo6srCzNZ8zPrpAuiYeHB8zNzWFkZARXV1fN8qCgIGzduhXh4eHw9PREly5d8OjRoxLbGDx4MFatWoVRo0bB09MT3t7emq/+yGQyTJ8+HT/++CNWrVoFqVQKV1dXjBkzpvQ3hV6JzwMlKmehoaFo0KABhgwZUtWl1Dpz585Fz549+T1KqhScwiWiGuHJkye4c+dOsYuQiCoKA5SIXnv379/HJ598Ak9PT83XcogqGqdwiYiI9MARKBERkR4YoERERHpggBIREemBAUpERKQHBigREZEe/j+wxtAFbcSlDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 11,\n",
       " 'min_samples_split': 19,\n",
       " 'min_samples_leaf': 15}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=11, min_samples_leaf=15,\n",
       "                       min_samples_split=19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=sklearn.tree.DecisionTreeClassifier(criterion=trial.params['criterion'],\n",
    "                                       max_depth=trial.params['max_depth'],\n",
    "                                        min_samples_split=trial.params['min_samples_split'],\n",
    "                                        min_samples_leaf=trial.params['min_samples_leaf'])\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17772  1406]\n",
      " [  718 18460]]\n",
      "0.9446240483887788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.93      0.94     19178\n",
      "         1.0       0.93      0.96      0.95     19178\n",
      "\n",
      "    accuracy                           0.94     38356\n",
      "   macro avg       0.95      0.94      0.94     38356\n",
      "weighted avg       0.95      0.94      0.94     38356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=dt.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:26:46,189]\u001b[0m A new study created in memory with name: no-name-ba10ffd9-6ab7-4dbf-ab02-d71c4e90ba23\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:26:46] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:155: Using tree method: 2\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[20:26:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[20:26:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:55] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:56] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:26:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:26:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:26:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[20:26:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:17] ======== Monitor (0): Learner ========\n",
      "[20:27:17] Configure: 7e-06s, 1 calls @ 7us\n",
      "\n",
      "[20:27:17] EvalOneIter: 0.00223s, 100 calls @ 2230us\n",
      "\n",
      "[20:27:17] GetGradient: 0.147407s, 100 calls @ 147407us\n",
      "\n",
      "[20:27:17] PredictRaw: 26.594s, 100 calls @ 26594008us\n",
      "\n",
      "[20:27:17] UpdateOneIter: 31.0861s, 100 calls @ 31086050us\n",
      "\n",
      "[20:27:17] ======== Monitor (0): GBTree ========\n",
      "[20:27:17] BoostNewTrees: 4.36083s, 100 calls @ 4360829us\n",
      "\n",
      "[20:27:17] ======== Monitor (0): TreePruner ========\n",
      "[20:27:17] PrunerUpdate: 0.131405s, 100 calls @ 131405us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:27:20,848]\u001b[0m Trial 0 finished with value: 0.9700942764411932 and parameters: {'booster': 'dart', 'lambda': 0.2314536488069119, 'alpha': 0.5356969828171508, 'subsample': 0.2233809975885803, 'colsample_bytree': 0.21425725162163123}. Best is trial 0 with value: 0.9700942764411932.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:20] ======== Monitor (0): Learner ========\n",
      "[20:27:20] Configure: 0.001417s, 1 calls @ 1417us\n",
      "\n",
      "[20:27:20] ======== Monitor (0): GBTree ========\n",
      "[20:27:20] ======== Monitor (0): TreePruner ========\n",
      "[20:27:20] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:155: Using tree method: 2\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:36] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:37] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:37] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:37] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:37] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:38] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:38] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:38] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:38] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:39] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[20:27:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[20:27:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[20:27:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[20:27:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:27:48] ======== Monitor (0): Learner ========\n",
      "[20:27:48] Configure: 0.001635s, 1 calls @ 1635us\n",
      "\n",
      "[20:27:48] EvalOneIter: 0.000973s, 100 calls @ 973us\n",
      "\n",
      "[20:27:48] GetGradient: 0.135452s, 100 calls @ 135452us\n",
      "\n",
      "[20:27:48] PredictRaw: 24.0803s, 100 calls @ 24080338us\n",
      "\n",
      "[20:27:48] UpdateOneIter: 27.7749s, 100 calls @ 27774934us\n",
      "\n",
      "[20:27:48] ======== Monitor (0): GBTree ========\n",
      "[20:27:48] BoostNewTrees: 3.48528s, 100 calls @ 3485281us\n",
      "\n",
      "[20:27:48] ======== Monitor (0): TreePruner ========\n",
      "[20:27:48] PrunerUpdate: 0.129945s, 100 calls @ 129945us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:27:52,141]\u001b[0m Trial 1 finished with value: 0.9577559115965174 and parameters: {'booster': 'dart', 'lambda': 0.1926923393617142, 'alpha': 0.25278272699311505, 'subsample': 0.18336763130737255, 'colsample_bytree': 0.1573381774810405}. Best is trial 0 with value: 0.9700942764411932.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:52] ======== Monitor (0): Learner ========\n",
      "[20:27:52] Configure: 0.001365s, 1 calls @ 1365us\n",
      "\n",
      "[20:27:52] ======== Monitor (0): GBTree ========\n",
      "[20:27:52] ======== Monitor (0): TreePruner ========\n",
      "[20:27:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bytree\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:27:53,629]\u001b[0m Trial 2 finished with value: 0.8943511411055587 and parameters: {'booster': 'gblinear', 'lambda': 0.9065290861020008, 'alpha': 0.20600380386343028, 'subsample': 0.13746158450789203, 'colsample_bytree': 0.2563264353664062}. Best is trial 0 with value: 0.9700942764411932.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:53] ======== Monitor (0): Learner ========\n",
      "[20:27:53] Configure: 0.001342s, 1 calls @ 1342us\n",
      "\n",
      "[20:27:53] EvalOneIter: 0.001057s, 100 calls @ 1057us\n",
      "\n",
      "[20:27:53] GetGradient: 0.160626s, 100 calls @ 160626us\n",
      "\n",
      "[20:27:53] PredictRaw: 0.223103s, 100 calls @ 223103us\n",
      "\n",
      "[20:27:53] UpdateOneIter: 1.16949s, 100 calls @ 1169489us\n",
      "\n",
      "[20:27:53] ======== Monitor (0): GBLinear ========\n",
      "[20:27:53] DoBoost: 0.78351s, 100 calls @ 783510us\n",
      "\n",
      "[20:27:53] PredictBatch: 0.222757s, 100 calls @ 222757us\n",
      "\n",
      "[20:27:53] PredictBatchInternal: 0.222433s, 100 calls @ 222433us\n",
      "\n",
      "[20:27:53] ======== Monitor (0): Learner ========\n",
      "[20:27:53] Configure: 0.000567s, 1 calls @ 567us\n",
      "\n",
      "[20:27:53] ======== Monitor (0): GBLinear ========\n",
      "[20:27:53] PredictBatch: 0.001739s, 1 calls @ 1739us\n",
      "\n",
      "[20:27:53] PredictBatchInternal: 0.001735s, 1 calls @ 1735us\n",
      "\n",
      "[20:27:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bytree\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:27:54,756]\u001b[0m Trial 3 finished with value: 0.5 and parameters: {'booster': 'gblinear', 'lambda': 0.04105839690981179, 'alpha': 0.5026293518659649, 'subsample': 0.37248783887211845, 'colsample_bytree': 0.2906849746775614}. Best is trial 0 with value: 0.9700942764411932.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:54] ======== Monitor (0): Learner ========\n",
      "[20:27:54] Configure: 0.002169s, 1 calls @ 2169us\n",
      "\n",
      "[20:27:54] EvalOneIter: 0.000996s, 100 calls @ 996us\n",
      "\n",
      "[20:27:54] GetGradient: 0.143455s, 100 calls @ 143455us\n",
      "\n",
      "[20:27:54] PredictRaw: 0.209299s, 100 calls @ 209299us\n",
      "\n",
      "[20:27:54] UpdateOneIter: 0.843767s, 100 calls @ 843767us\n",
      "\n",
      "[20:27:54] ======== Monitor (0): GBLinear ========\n",
      "[20:27:54] DoBoost: 0.488035s, 100 calls @ 488035us\n",
      "\n",
      "[20:27:54] PredictBatch: 0.209006s, 100 calls @ 209006us\n",
      "\n",
      "[20:27:54] PredictBatchInternal: 0.208714s, 100 calls @ 208714us\n",
      "\n",
      "[20:27:54] ======== Monitor (0): Learner ========\n",
      "[20:27:54] Configure: 0.000624s, 1 calls @ 624us\n",
      "\n",
      "[20:27:54] ======== Monitor (0): GBLinear ========\n",
      "[20:27:54] PredictBatch: 0.001709s, 1 calls @ 1709us\n",
      "\n",
      "[20:27:54] PredictBatchInternal: 0.001705s, 1 calls @ 1705us\n",
      "\n",
      "[20:27:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bytree\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:27:56,166]\u001b[0m Trial 4 finished with value: 0.8943511411055587 and parameters: {'booster': 'gblinear', 'lambda': 0.2323083300081686, 'alpha': 0.16024462427685218, 'subsample': 0.27854365783642565, 'colsample_bytree': 0.1262551709842991}. Best is trial 0 with value: 0.9700942764411932.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:56] ======== Monitor (0): Learner ========\n",
      "[20:27:56] Configure: 0.001325s, 1 calls @ 1325us\n",
      "\n",
      "[20:27:56] EvalOneIter: 0.001043s, 100 calls @ 1043us\n",
      "\n",
      "[20:27:56] GetGradient: 0.140858s, 100 calls @ 140858us\n",
      "\n",
      "[20:27:56] PredictRaw: 0.192169s, 100 calls @ 192169us\n",
      "\n",
      "[20:27:56] UpdateOneIter: 1.09401s, 100 calls @ 1094008us\n",
      "\n",
      "[20:27:56] ======== Monitor (0): GBLinear ========\n",
      "[20:27:56] DoBoost: 0.758803s, 100 calls @ 758803us\n",
      "\n",
      "[20:27:56] PredictBatch: 0.191849s, 100 calls @ 191849us\n",
      "\n",
      "[20:27:56] PredictBatchInternal: 0.191558s, 100 calls @ 191558us\n",
      "\n",
      "[20:27:56] ======== Monitor (0): Learner ========\n",
      "[20:27:56] Configure: 0.000627s, 1 calls @ 627us\n",
      "\n",
      "[20:27:56] ======== Monitor (0): GBLinear ========\n",
      "[20:27:56] PredictBatch: 0.001737s, 1 calls @ 1737us\n",
      "\n",
      "[20:27:56] PredictBatchInternal: 0.001734s, 1 calls @ 1734us\n",
      "\n",
      "[20:27:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bytree\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:27:57,262]\u001b[0m Trial 5 finished with value: 0.5 and parameters: {'booster': 'gblinear', 'lambda': 0.8318107448011184, 'alpha': 0.9595425780392538, 'subsample': 0.41895680879019437, 'colsample_bytree': 0.3072459610199601}. Best is trial 0 with value: 0.9700942764411932.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:57] ======== Monitor (0): Learner ========\n",
      "[20:27:57] Configure: 0.002176s, 1 calls @ 2176us\n",
      "\n",
      "[20:27:57] EvalOneIter: 0.001024s, 100 calls @ 1024us\n",
      "\n",
      "[20:27:57] GetGradient: 0.141773s, 100 calls @ 141773us\n",
      "\n",
      "[20:27:57] PredictRaw: 0.197893s, 100 calls @ 197893us\n",
      "\n",
      "[20:27:57] UpdateOneIter: 0.810678s, 100 calls @ 810678us\n",
      "\n",
      "[20:27:57] ======== Monitor (0): GBLinear ========\n",
      "[20:27:57] DoBoost: 0.467965s, 100 calls @ 467965us\n",
      "\n",
      "[20:27:57] PredictBatch: 0.19758s, 100 calls @ 197580us\n",
      "\n",
      "[20:27:57] PredictBatchInternal: 0.197299s, 100 calls @ 197299us\n",
      "\n",
      "[20:27:57] ======== Monitor (0): Learner ========\n",
      "[20:27:57] Configure: 0.000619s, 1 calls @ 619us\n",
      "\n",
      "[20:27:57] ======== Monitor (0): GBLinear ========\n",
      "[20:27:57] PredictBatch: 0.001757s, 1 calls @ 1757us\n",
      "\n",
      "[20:27:57] PredictBatchInternal: 0.001754s, 1 calls @ 1754us\n",
      "\n",
      "[20:27:57] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:155: Using tree method: 2\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:57] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:58] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:27:59] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:00] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:01] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:02] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:03] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] ======== Monitor (0): Learner ========\n",
      "[20:28:04] Configure: 0.001538s, 1 calls @ 1538us\n",
      "\n",
      "[20:28:04] EvalOneIter: 0.000931s, 100 calls @ 931us\n",
      "\n",
      "[20:28:04] GetGradient: 0.153053s, 100 calls @ 153053us\n",
      "\n",
      "[20:28:04] PredictRaw: 0.538536s, 100 calls @ 538536us\n",
      "\n",
      "[20:28:04] UpdateOneIter: 6.78447s, 100 calls @ 6784471us\n",
      "\n",
      "[20:28:04] ======== Monitor (0): GBTree ========\n",
      "[20:28:04] BoostNewTrees: 6.08645s, 100 calls @ 6086447us\n",
      "\n",
      "[20:28:04] CommitModel: 0.000155s, 100 calls @ 155us\n",
      "\n",
      "[20:28:04] ======== Monitor (0): TreePruner ========\n",
      "[20:28:04] PrunerUpdate: 0.11373s, 100 calls @ 113730us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:28:04,508]\u001b[0m Trial 6 finished with value: 0.9752717531296687 and parameters: {'booster': 'gbtree', 'lambda': 0.8726548556123835, 'alpha': 0.658659200164908, 'subsample': 0.2600165125235496, 'colsample_bytree': 0.4969552325949049}. Best is trial 6 with value: 0.9752717531296687.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:04] ======== Monitor (0): Learner ========\n",
      "[20:28:04] Configure: 0.000942s, 1 calls @ 942us\n",
      "\n",
      "[20:28:04] ======== Monitor (0): GBTree ========\n",
      "[20:28:04] ======== Monitor (0): TreePruner ========\n",
      "[20:28:04] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:155: Using tree method: 2\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:04] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:06] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 106 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:25] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:26] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:27] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:28] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:29] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:30] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:31] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:32] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:33] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:34] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:35] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:909: drop 0 trees, weight = 1\n",
      "[20:28:35] ======== Monitor (0): Learner ========\n",
      "[20:28:35] Configure: 0.001279s, 1 calls @ 1279us\n",
      "\n",
      "[20:28:35] EvalOneIter: 0.000983s, 100 calls @ 983us\n",
      "\n",
      "[20:28:35] GetGradient: 0.136632s, 100 calls @ 136632us\n",
      "\n",
      "[20:28:35] PredictRaw: 25.861s, 100 calls @ 25860955us\n",
      "\n",
      "[20:28:35] UpdateOneIter: 31.173s, 100 calls @ 31172981us\n",
      "\n",
      "[20:28:35] ======== Monitor (0): GBTree ========\n",
      "[20:28:35] BoostNewTrees: 5.10182s, 100 calls @ 5101818us\n",
      "\n",
      "[20:28:35] ======== Monitor (0): TreePruner ========\n",
      "[20:28:35] PrunerUpdate: 0.136427s, 100 calls @ 136427us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:28:39,116]\u001b[0m Trial 7 finished with value: 0.973713358405028 and parameters: {'booster': 'dart', 'lambda': 0.8711880014953811, 'alpha': 0.4314530391833857, 'subsample': 0.31797707192489916, 'colsample_bytree': 0.3182869289746802}. Best is trial 6 with value: 0.9752717531296687.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:39] ======== Monitor (0): Learner ========\n",
      "[20:28:39] Configure: 0.001356s, 1 calls @ 1356us\n",
      "\n",
      "[20:28:39] ======== Monitor (0): GBTree ========\n",
      "[20:28:39] ======== Monitor (0): TreePruner ========\n",
      "[20:28:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"colsample_bytree\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:28:40,488]\u001b[0m Trial 8 finished with value: 0.9124336716294884 and parameters: {'booster': 'gblinear', 'lambda': 0.1976375302749832, 'alpha': 0.06571043694772942, 'subsample': 0.26305888609911615, 'colsample_bytree': 0.4205583935440911}. Best is trial 6 with value: 0.9752717531296687.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:40] ======== Monitor (0): Learner ========\n",
      "[20:28:40] Configure: 0.00166s, 1 calls @ 1660us\n",
      "\n",
      "[20:28:40] EvalOneIter: 0.001018s, 100 calls @ 1018us\n",
      "\n",
      "[20:28:40] GetGradient: 0.139672s, 100 calls @ 139672us\n",
      "\n",
      "[20:28:40] PredictRaw: 0.198175s, 100 calls @ 198175us\n",
      "\n",
      "[20:28:40] UpdateOneIter: 1.06865s, 100 calls @ 1068653us\n",
      "\n",
      "[20:28:40] ======== Monitor (0): GBLinear ========\n",
      "[20:28:40] DoBoost: 0.728291s, 100 calls @ 728291us\n",
      "\n",
      "[20:28:40] PredictBatch: 0.197865s, 100 calls @ 197865us\n",
      "\n",
      "[20:28:40] PredictBatchInternal: 0.197562s, 100 calls @ 197562us\n",
      "\n",
      "[20:28:40] ======== Monitor (0): Learner ========\n",
      "[20:28:40] Configure: 0.000619s, 1 calls @ 619us\n",
      "\n",
      "[20:28:40] ======== Monitor (0): GBLinear ========\n",
      "[20:28:40] PredictBatch: 0.001726s, 1 calls @ 1726us\n",
      "\n",
      "[20:28:40] PredictBatchInternal: 0.001719s, 1 calls @ 1719us\n",
      "\n",
      "[20:28:40] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:155: Using tree method: 2\n",
      "[20:28:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:40] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:41] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:42] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:43] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:44] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:45] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:46] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:47] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:47] ======== Monitor (0): Learner ========\n",
      "[20:28:47] Configure: 0.001524s, 1 calls @ 1524us\n",
      "\n",
      "[20:28:47] EvalOneIter: 0.000966s, 100 calls @ 966us\n",
      "\n",
      "[20:28:47] GetGradient: 0.147899s, 100 calls @ 147899us\n",
      "\n",
      "[20:28:47] PredictRaw: 0.534831s, 100 calls @ 534831us\n",
      "\n",
      "[20:28:47] UpdateOneIter: 6.5126s, 100 calls @ 6512599us\n",
      "\n",
      "[20:28:47] ======== Monitor (0): GBTree ========\n",
      "[20:28:47] BoostNewTrees: 5.8228s, 100 calls @ 5822799us\n",
      "\n",
      "[20:28:47] CommitModel: 0.000133s, 100 calls @ 133us\n",
      "\n",
      "[20:28:47] ======== Monitor (0): TreePruner ========\n",
      "[20:28:47] PrunerUpdate: 0.116802s, 100 calls @ 116802us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-06 20:28:47,522]\u001b[0m Trial 9 finished with value: 0.975928597187162 and parameters: {'booster': 'gbtree', 'lambda': 0.03698372506706345, 'alpha': 0.3565967456483075, 'subsample': 0.22066640010407823, 'colsample_bytree': 0.48930969405740266}. Best is trial 9 with value: 0.975928597187162.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:47] ======== Monitor (0): Learner ========\n",
      "[20:28:47] Configure: 0.001313s, 1 calls @ 1313us\n",
      "\n",
      "[20:28:47] ======== Monitor (0): GBTree ========\n",
      "[20:28:47] ======== Monitor (0): TreePruner ========\n"
     ]
    }
   ],
   "source": [
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    classifier_name = 'xgb'\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    param  = {\n",
    "        'verbosity' : 3,\n",
    "        'booster' : trial.suggest_categorical('booster' , ['dart' , 'gbtree','gblinear']),\n",
    "        'lambda' : trial.suggest_float('lambda' , 1e-4 , 1),\n",
    "        'alpha' :trial.suggest_float('alpha' , 1e-4 , 1),\n",
    "        'subsample' : trial.suggest_float('subsample' , .1,.5),\n",
    "        'colsample_bytree' : trial.suggest_float('colsample_bytree' , .1 ,.5)\n",
    "        \n",
    "    }\n",
    "   \n",
    "    if param['booster'] in ['gbtree' , 'dart']:\n",
    "        param['gamma'] :trial.suggest_float('gamma' , 1e-3 , 4 )\n",
    "        param['eta'] : trial.suggest_float('eta' , .001 ,5 )\n",
    "    classifier_obj = xgb.XGBClassifier(**param)        \n",
    "    # Step 3: Scoring method:\n",
    "    classifier_obj.fit(X_train, y_train)\n",
    "    accuracy  = classifier_obj.score(X_train, y_train)\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree',\n",
       " 'lambda': 0.03698372506706345,\n",
       " 'alpha': 0.3565967456483075,\n",
       " 'subsample': 0.22066640010407823,\n",
       " 'colsample_bytree': 0.48930969405740266}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "param=trial.params\n",
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:48] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:155: Using tree method: 2\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:48] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:49] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:50] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:51] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:52] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:53] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:54] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20:28:55] ======== Monitor (0): Learner ========\n",
      "[20:28:55] Configure: 0.001425s, 1 calls @ 1425us\n",
      "\n",
      "[20:28:55] EvalOneIter: 0.001008s, 100 calls @ 1008us\n",
      "\n",
      "[20:28:55] GetGradient: 0.137612s, 100 calls @ 137612us\n",
      "\n",
      "[20:28:55] PredictRaw: 0.523051s, 100 calls @ 523051us\n",
      "\n",
      "[20:28:55] UpdateOneIter: 6.45117s, 100 calls @ 6451172us\n",
      "\n",
      "[20:28:55] ======== Monitor (0): GBTree ========\n",
      "[20:28:55] BoostNewTrees: 5.7845s, 100 calls @ 5784500us\n",
      "\n",
      "[20:28:55] CommitModel: 0.000146s, 100 calls @ 146us\n",
      "\n",
      "[20:28:55] ======== Monitor (0): TreePruner ========\n",
      "[20:28:55] PrunerUpdate: 0.117675s, 100 calls @ 117675us\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.3565967456483075, base_score=0.5, booster='gbtree',\n",
       "              callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n",
       "              colsample_bytree=0.48930969405740266, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', lambda=0.03698372506706345,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_obj = xgb.XGBClassifier(**param)\n",
    "xgb_obj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17794  1384]\n",
      " [  707 18471]]\n",
      "0.9454844092188966\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.93      0.94     19178\n",
      "         1.0       0.93      0.96      0.95     19178\n",
      "\n",
      "    accuracy                           0.95     38356\n",
      "   macro avg       0.95      0.95      0.95     38356\n",
      "weighted avg       0.95      0.95      0.95     38356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=xgb_obj.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "### From the above experiments we can select some hyperparameters for each algos\n",
    "### 1.Logistic Regression: C\n",
    "### 2.Random Forest: min_samples_split,min_samples_leaf\n",
    "### 3.SVC: kernel\n",
    "### 4.Naive Bayes: var_smoothing\n",
    "### 5.Descision Tree: max_depth\n",
    "### 6.XGBoost: subsample,alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a function to select the best model after hyperparameter tuning giving best accuracy and best auc_roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-08 12:59:18,236]\u001b[0m A new study created in memory with name: no-name-2709f8e5-c16f-44ae-add2-28bf7bc844bb\u001b[0m\n",
      "\u001b[32m[I 2022-08-08 12:59:21,556]\u001b[0m Trial 0 finished with values: [0.9518186619685386, 0.9837935688186763] and parameters: {'classifier': 'decision-tree', 'max_depth': 8}. \u001b[0m\n",
      "\u001b[32m[I 2022-08-08 13:00:32,250]\u001b[0m Trial 1 finished with values: [0.9652904155810575, 0.9941830641701124] and parameters: {'classifier': 'xgb', 'alpha': 0.1765170046214821, 'subsample': 0.2442955003517441}. \u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-08 13:03:37,402]\u001b[0m Trial 2 finished with values: [0.9587992527659316, 0.9934338841595831] and parameters: {'classifier': 'RandomForest', 'min_samples_split': 12, 'min_samples_leaf': 14}. \u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-08 13:03:42,738]\u001b[0m Trial 3 finished with values: [0.9249782053611574, 0.9739038224456872] and parameters: {'classifier': 'LogReg', 'C': 9.643526439494615}. \u001b[0m\n",
      "\u001b[32m[I 2022-08-08 13:03:48,141]\u001b[0m Trial 4 finished with values: [0.9436145507289005, 0.9797957681500036] and parameters: {'classifier': 'decision-tree', 'max_depth': 6}. \u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "\u001b[32m[I 2022-08-08 13:07:13,113]\u001b[0m Trial 5 finished with values: [0.9612591905283266, 0.994006861062062] and parameters: {'classifier': 'RandomForest', 'min_samples_split': 2, 'min_samples_leaf': 9}. \u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-08 13:25:08,258]\u001b[0m Trial 6 finished with values: [0.9387848179961708, 0.9818604698283963] and parameters: {'classifier': 'SVC', 'kernel': 'poly'}. \u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-08 13:25:10,025]\u001b[0m Trial 7 finished with values: [0.9177915536358601, 0.9602784308012545] and parameters: {'classifier': 'NaiveBayes', 'var_smoothing': 0.0008954815584310066}. \u001b[0m\n",
      "\u001b[32m[I 2022-08-08 13:25:14,144]\u001b[0m Trial 8 finished with values: [0.9435759118479082, 0.9797925999952994] and parameters: {'classifier': 'decision-tree', 'max_depth': 6}. \u001b[0m\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "\u001b[32m[I 2022-08-08 13:25:16,614]\u001b[0m Trial 9 finished with values: [0.9249782053611574, 0.9739038290805662] and parameters: {'classifier': 'LogReg', 'C': 9.038475064785358}. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "X_train=train.drop(labels=\"phishing\",axis=1)\n",
    "X_test=test.drop(labels=\"phishing\",axis=1)\n",
    "y_train=train[[\"phishing\"]]\n",
    "y_test=test[[\"phishing\"]]\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"LogReg\", \"RandomForest\",\"SVC\",\"NaiveBayes\",\n",
    "                                                               \"decision-tree\",\"xgb\"])\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    if classifier_name == 'LogReg':\n",
    "        C = trial.suggest_uniform('C', 0.01, 10)\n",
    "        classifier_obj = linear_model.LogisticRegression(C=C)\n",
    "    \n",
    "    elif classifier_name == 'RandomForest':\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n",
    "        classifier_obj = sklearn.ensemble.RandomForestClassifier(min_samples_split=min_samples_split,\n",
    "                                                                 min_samples_leaf=min_samples_leaf)\n",
    "    \n",
    "    elif classifier_name == 'SVC':\n",
    "        kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "        classifier_obj = sklearn.svm.SVC(kernel=kernel)\n",
    "        \n",
    "    elif classifier_name == 'NaiveBayes':\n",
    "        var_smoothing=trial.suggest_float(\"var_smoothing\", 1e-4, 0.3, log=True)\n",
    "        classifier_obj = sklearn.naive_bayes.GaussianNB(var_smoothing=var_smoothing)\n",
    "    elif classifier_name == 'decision-tree':\n",
    "        max_depth = trial.suggest_int('max_depth', 5, X_train.shape[1])\n",
    "        classifier_obj = sklearn.tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "    \n",
    "    elif classifier_name == 'xgb':\n",
    "        alpha =trial.suggest_float('alpha' , 1e-4 , 1)\n",
    "        subsample =trial.suggest_float('subsample' , .1,.5)\n",
    "        classifier_obj = xgb.XGBClassifier(alpha=alpha,subsample=subsample)\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    score = model_selection.cross_val_score(classifier_obj, X_train, y_train, cv=3)\n",
    "    roc_auc_score = model_selection.cross_val_score(classifier_obj, X_train, y_train, scoring=\"roc_auc\")\n",
    "    accuracy = score.mean()\n",
    "    roc_auc = roc_auc_score.mean()\n",
    "    return accuracy,roc_auc\n",
    "\n",
    "# Step 4: Running it\n",
    "sampler = optuna.samplers.NSGAIISampler()\n",
    "study = optuna.create_study(directions=[\"maximize\", \"maximize\"], sampler=sampler)\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and Auc roc score: [0.9652904155810575, 0.9941830641701124]\n",
      "Best hyperparameters: {'classifier': 'xgb', 'alpha': 0.1765170046214821, 'subsample': 0.2442955003517441}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trials\n",
    "trial[0]\n",
    "print('Accuracy and Auc roc score: {}'.format(trial[0].values))\n",
    "print(\"Best hyperparameters: {}\".format(trial[0].params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': 'xgb',\n",
       " 'alpha': 0.1765170046214821,\n",
       " 'subsample': 0.2442955003517441}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trials\n",
    "param=trial[0].params\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:36:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"classifier\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.1765170046214821, base_score=0.5, booster='gbtree',\n",
       "              callbacks=None, classifier='xgb', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_obj = xgb.XGBClassifier(**param)\n",
    "xgb_obj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17897  1281]\n",
      " [  629 18549]]\n",
      "0.9502033580143915\n",
      "0.9502033580143914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.93      0.95     19178\n",
      "         1.0       0.94      0.97      0.95     19178\n",
      "\n",
      "    accuracy                           0.95     38356\n",
      "   macro avg       0.95      0.95      0.95     38356\n",
      "weighted avg       0.95      0.95      0.95     38356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=xgb_obj.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
